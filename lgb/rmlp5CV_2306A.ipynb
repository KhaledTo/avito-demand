{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/tunguz/bow-meta-text-and-dense-features-lgbm-clone?scriptVersionId=3540839\n",
    "\n",
    "# Models Packages\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time, gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import pymorphy2\n",
    "import nltk, re\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "import os, random\n",
    "os.environ['PYTHONHASHSEED'] = '10000'\n",
    "np.random.seed(10001)\n",
    "random.seed(10002)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=5, inter_op_parallelism_threads=1)\n",
    "from keras import backend\n",
    "tf.set_random_seed(10003)\n",
    "backend.set_session(tf.Session(graph=tf.get_default_graph(), config=session_conf))\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dropout, Dense, concatenate, GRU, Embedding, Flatten, Activation, BatchNormalization, PReLU\n",
    "from keras.initializers import he_uniform\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.461143493652344e-05] Load Train/Test\n",
      "Train shape: 1503424 Rows, 16 Columns\n",
      "Test shape: 508438 Rows, 16 Columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True     465829\n",
       "False     42609\n",
       "Name: image_top_1, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#path = '../input/'\n",
    "path = \"/home/darragh/avito/data/\"\n",
    "#path = '/Users/dhanley2/Documents/avito/data/'\n",
    "path = '/home/ubuntu/avito/data/'\n",
    "start_time = time.time()\n",
    "full = False\n",
    "\n",
    "print('[{}] Load Train/Test'.format(time.time() - start_time))\n",
    "traindf = pd.read_csv(path + 'train.csv.zip', index_col = \"item_id\", parse_dates = [\"activation_date\"], compression = 'zip')\n",
    "traindex = traindf.index\n",
    "testdf = pd.read_csv(path + 'test.csv.zip', index_col = \"item_id\", parse_dates = [\"activation_date\"])\n",
    "testdex = testdf.index\n",
    "y = traindf.deal_probability.copy()\n",
    "traindf.drop(\"deal_probability\",axis=1, inplace=True)\n",
    "print('Train shape: {} Rows, {} Columns'.format(*traindf.shape))\n",
    "print('Test shape: {} Rows, {} Columns'.format(*testdf.shape))\n",
    "traindf['activation_date'].value_counts()\n",
    "\n",
    "(traindf['image_top_1'] == traindf['image_top_1']).value_counts()\n",
    "(testdf['image_top_1'] == testdf['image_top_1']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28.010176181793213] Create Validation Index\n",
      "[28.01868462562561] Combine Train and Test\n",
      "\n",
      "All Data shape: 2011862 Rows, 17 Columns\n"
     ]
    }
   ],
   "source": [
    "print('[{}] Create Validation Index'.format(time.time() - start_time))\n",
    "if full:\n",
    "    trnidx = (traindf.activation_date<=pd.to_datetime('2017-03-28')).values\n",
    "    validx = (traindf.activation_date>=pd.to_datetime('2017-03-29')).values\n",
    "else:\n",
    "    trnidx = (traindf.activation_date<=pd.to_datetime('2017-03-26')).values\n",
    "    validx = (traindf.activation_date>=pd.to_datetime('2017-03-27')).values\n",
    "\n",
    "print('[{}] Combine Train and Test'.format(time.time() - start_time))\n",
    "df = pd.concat([traindf,testdf],axis=0)\n",
    "del traindf,testdf\n",
    "gc.collect()\n",
    "df['idx'] = range(df.shape[0])\n",
    "print('\\nAll Data shape: {} Rows, {} Columns'.format(*df.shape))\n",
    "\n",
    "#print('[{}] Count NA row wise'.format(time.time() - start_time))\n",
    "#df['NA_count_rows'] = df.isnull().sum(axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28.975817680358887] Load meta image engineered features\n",
      "[48.48427700996399] Load translated image engineered features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('[{}] Load meta image engineered features'.format(time.time() - start_time))\n",
    "featimgmeta = pd.concat([pd.read_csv(path + '../features/img_features_%s.csv.gz'%(i)) for i in range(6)])\n",
    "featimgmeta.rename(columns = {'name':'image'}, inplace = True)\n",
    "featimgmeta['image'] = featimgmeta['image'].str.replace('.jpg', '')\n",
    "df = df.reset_index('item_id').merge(featimgmeta, on = ['image'], how = 'left').set_index('item_id')\n",
    "for col in featimgmeta.columns.values[1:]:\n",
    "    df[col].fillna(-1, inplace = True)\n",
    "    df[col].astype(np.float32, inplace = True)\n",
    "    \n",
    "print('[{}] Load translated image engineered features'.format(time.time() - start_time))\n",
    "feattrlten = pd.concat([pd.read_csv(path + '../features/translate_trn_en.csv.gz', compression = 'gzip'),\n",
    "                        pd.read_csv(path + '../features/translate_tst_en.csv.gz', compression = 'gzip')])\n",
    "# feattrlten = pd.concat([pd.read_pickle(path + '../features/translate_trn_en.pkl'),\n",
    "#                        pd.read_pickle(path + '../features/translate_tst_en.pkl')])\n",
    "feattrlten.fillna('', inplace = True)\n",
    "feattrlten['translation'] = feattrlten['title_translated'] + ' ' + feattrlten['param_1_translated'] + ' ' \\\n",
    "            + feattrlten['param_2_translated'] + ' ' + feattrlten['param_3_translated'] + ' '  \\\n",
    "            + feattrlten['category_name_translated'] + ' ' + feattrlten['parent_category_name_translated']\n",
    "feattrlten = feattrlten.set_index('item_id')[['translation']]\n",
    "feattrlten.head()\n",
    "df = pd.merge(df, feattrlten, left_index=True, right_index=True, how='left')\n",
    "del feattrlten\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77.41031122207642] Load other engineered features\n",
      "\n",
      "All Data shape: 2011862 Rows, 44 Columns\n"
     ]
    }
   ],
   "source": [
    "print('[{}] Load other engineered features'.format(time.time() - start_time))\n",
    "featlatlon = pd.read_csv(path + '../features/avito_region_city_features.csv') # https://www.kaggle.com/frankherfert/region-and-city-details-with-lat-lon-and-clusters\n",
    "featlatlon.drop(['city_region', 'city_region_id', 'region_id'], 1, inplace = True)\n",
    "featpop    = pd.read_csv(path + '../features/city_population_wiki_v3.csv') # https://www.kaggle.com/stecasasso/russian-city-population-from-wikipedia/comments\n",
    "featusrttl = pd.read_csv(path + '../features/user_agg.csv.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featusrcat = pd.read_csv(path + '../features/usercat_agg.csv.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featusrprd = pd.read_csv(path + '../features/user_activ_period_stats.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featrdgtxt = pd.read_csv(path + '../features/ridgeText5CV.csv.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "#featrdgtxts = pd.read_csv(path + '../features/ridgeTextStr5CV.csv.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featrdgimg = pd.read_csv(path + '../features/ridgeImg5CV.csv.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "#featrdgprc = pd.read_csv(path + '../features/price_category_ratios.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featrdgprc = pd.read_csv(path + '../features/price_seq_category_ratios.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featrdgprc.fillna(-1, inplace = True)\n",
    "featrdgrnk = pd.read_csv(path + '../features/price_rank_ratios0906.gz', compression = 'gzip') # created with R script and stemmer\n",
    "featrdgrnk.isnull().sum()\n",
    "featnumf = pd.read_csv(path + '../features/numericFeats.gz', compression = 'gzip') \n",
    "featnumf.fillna(0, inplace = True)\n",
    "featencfst = pd.read_csv(path + '../features/alldf_bayes_fest_1206.gz', compression = 'gzip')\n",
    "featprtfst = pd.read_csv(path + '../features/pratios_fest_1206.gz', compression = 'gzip')\n",
    "\n",
    "featprmenc = pd.read_csv(path + '../features/alldf_bayes_mean_param_1006.gz', compression = 'gzip') \n",
    "featprmtro = pd.read_csv(path + '../features/price_param_ratios1006.gz', compression = 'gzip') \n",
    "#featimgnet = pd.read_csv(path + '../features/imgnet_decode_feats.csv.gz', compression = 'gzip')\n",
    "featldlag  = pd.read_csv(path + '../features/pseq_leadlag_festivities_1906.gz', compression = 'gzip')\n",
    "\n",
    "featimgprc = pd.read_csv(path + '../features/price_imagetop1_ratios.gz', compression = 'gzip') # created with features/make/priceImgRatios2705.R\n",
    "featenc = pd.read_csv(path + '../features/alldf_bayes_mean.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featct  = pd.read_csv(path + '../features/alldf_count.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featusrttl.rename(columns={'title': 'all_titles'}, inplace = True)\n",
    "df = df.reset_index().merge(featpop, on = 'city', how = 'left')\n",
    "df = df.merge(featlatlon, on = ['city', 'region'], how = 'left')\n",
    "df['population'].fillna(-1, inplace = True)\n",
    "df = df.set_index('item_id')\n",
    "keep = ['user_id', 'all_titles', 'user_avg_price', 'user_ad_ct']\n",
    "df = df.reset_index().merge(featusrttl[keep], on = 'user_id').set_index('item_id')\n",
    "keep = ['user_id', 'parent_category_name', 'usercat_avg_price', 'usercat_ad_ct']\n",
    "gc.collect()\n",
    "df = df.reset_index().merge(featusrcat[keep], on = ['user_id', 'parent_category_name']).set_index('item_id')\n",
    "keep = ['user_id', 'user_activ_sum', 'user_activ_mean', 'user_activ_var']\n",
    "gc.collect()\n",
    "df = df.reset_index().merge(featusrprd[keep], on = ['user_id'], how = 'left').set_index('item_id')\n",
    "#df = df.reset_index().merge(featimgnet, on = ['item_id'], how = 'left').set_index('item_id')\n",
    "\n",
    "print('\\nAll Data shape: {} Rows, {} Columns'.format(*df.shape))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[215.80405712127686] Resort data correctly\n",
      "[221.35363268852234] Create folds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>item_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>parent_category_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>param_1</th>\n",
       "      <th>param_2</th>\n",
       "      <th>param_3</th>\n",
       "      <th>...</th>\n",
       "      <th>seq_seqsort_lead</th>\n",
       "      <th>seq_seqsort_lag_diff</th>\n",
       "      <th>seq_seqsort_lead_diff</th>\n",
       "      <th>item_seq_repeat_postings</th>\n",
       "      <th>category_repeat_postings</th>\n",
       "      <th>price_min_sequence</th>\n",
       "      <th>price_min_sequence_diff</th>\n",
       "      <th>seq_min_sequence</th>\n",
       "      <th>seq_min_sequence_diff</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b912c3c6a6ad</td>\n",
       "      <td>e00f8ff2eaf9</td>\n",
       "      <td>Свердловская область</td>\n",
       "      <td>Екатеринбург</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Товары для детей и игрушки</td>\n",
       "      <td>Постельные принадлежности</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-999</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>-3600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2dac0150717d</td>\n",
       "      <td>39aeb48f0017</td>\n",
       "      <td>Самарская область</td>\n",
       "      <td>Самара</td>\n",
       "      <td>Для дома и дачи</td>\n",
       "      <td>Мебель и интерьер</td>\n",
       "      <td>Другое</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ba83aefab5dc</td>\n",
       "      <td>91e2f88dd6e3</td>\n",
       "      <td>Ростовская область</td>\n",
       "      <td>Ростов-на-Дону</td>\n",
       "      <td>Бытовая электроника</td>\n",
       "      <td>Аудио и видео</td>\n",
       "      <td>Видео, DVD и Blu-ray плееры</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>02996f1dd2ea</td>\n",
       "      <td>bf5cccea572d</td>\n",
       "      <td>Татарстан</td>\n",
       "      <td>Набережные Челны</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Товары для детей и игрушки</td>\n",
       "      <td>Автомобильные кресла</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>-999</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>20</td>\n",
       "      <td>266</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7c90be56d2ab</td>\n",
       "      <td>ef50846afc0b</td>\n",
       "      <td>Волгоградская область</td>\n",
       "      <td>Волгоград</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили</td>\n",
       "      <td>С пробегом</td>\n",
       "      <td>ВАЗ (LADA)</td>\n",
       "      <td>2110</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 206 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index       item_id       user_id                 region              city  \\\n",
       "0      0  b912c3c6a6ad  e00f8ff2eaf9   Свердловская область      Екатеринбург   \n",
       "1      1  2dac0150717d  39aeb48f0017      Самарская область            Самара   \n",
       "2      2  ba83aefab5dc  91e2f88dd6e3     Ростовская область    Ростов-на-Дону   \n",
       "3      3  02996f1dd2ea  bf5cccea572d              Татарстан  Набережные Челны   \n",
       "4      4  7c90be56d2ab  ef50846afc0b  Волгоградская область         Волгоград   \n",
       "\n",
       "  parent_category_name               category_name  \\\n",
       "0          Личные вещи  Товары для детей и игрушки   \n",
       "1      Для дома и дачи           Мебель и интерьер   \n",
       "2  Бытовая электроника               Аудио и видео   \n",
       "3          Личные вещи  Товары для детей и игрушки   \n",
       "4            Транспорт                  Автомобили   \n",
       "\n",
       "                       param_1     param_2 param_3  ...  seq_seqsort_lead  \\\n",
       "0    Постельные принадлежности         NaN     NaN  ...                -1   \n",
       "1                       Другое         NaN     NaN  ...                -1   \n",
       "2  Видео, DVD и Blu-ray плееры         NaN     NaN  ...                10   \n",
       "3         Автомобильные кресла         NaN     NaN  ...                -1   \n",
       "4                   С пробегом  ВАЗ (LADA)    2110  ...                -1   \n",
       "\n",
       "  seq_seqsort_lag_diff  seq_seqsort_lead_diff  item_seq_repeat_postings  \\\n",
       "0                    1                   -999                         1   \n",
       "1                 -999                   -999                         1   \n",
       "2                    1                     -1                         1   \n",
       "3                    3                   -999                         1   \n",
       "4                 -999                   -999                         1   \n",
       "\n",
       "  category_repeat_postings price_min_sequence price_min_sequence_diff  \\\n",
       "0                        3             4000.0                 -3600.0   \n",
       "1                        1             3000.0                     0.0   \n",
       "2                        1             2000.0                  2000.0   \n",
       "3                        7                0.0                  2200.0   \n",
       "4                        1            40000.0                     0.0   \n",
       "\n",
       "   seq_min_sequence  seq_min_sequence_diff  fold  \n",
       "0                 1                      1     4  \n",
       "1                19                      0     3  \n",
       "2                 1                      8     1  \n",
       "3                20                    266     3  \n",
       "4                 3                      0     0  \n",
       "\n",
       "[5 rows x 206 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('[{}] Resort data correctly'.format(time.time() - start_time))\n",
    "df.sort_values('idx', inplace = True)\n",
    "df.drop(['idx'], axis=1,inplace=True)\n",
    "df.reset_index(inplace = True)\n",
    "df.head()\n",
    "df = pd.concat([df.reset_index(),featenc, featct, featrdgtxt, featrdgprc, featimgprc, featrdgrnk, featnumf, featprmenc, featprmtro, featencfst, featprtfst, featldlag],axis=1)\n",
    "#df['ridge_txt'] = featrdgtxt['ridge_preds'].values\n",
    "#df = pd.concat([df.reset_index(),featenc, featct, ],axis=1)\n",
    "\n",
    "print('[{}] Create folds'.format(time.time() - start_time))\n",
    "foldls = [[\"2017-03-15\", \"2017-03-16\", \"2017-03-17\"], \\\n",
    "       [\"2017-03-18\", \"2017-03-19\", \"2017-03-20\"], \\\n",
    "       [\"2017-03-21\", \"2017-03-22\", \"2017-03-23\"], \\\n",
    "       [\"2017-03-24\", \"2017-03-25\", \"2017-03-26\"], \\\n",
    "        [\"2017-03-27\", \"2017-03-28\", \"2017-03-29\", \\\n",
    "            \"2017-03-30\", \"2017-03-31\", \"2017-04-01\", \\\n",
    "            \"2017-04-02\", \"2017-04-03\",\"2017-04-07\"]]\n",
    "foldls = [[pd.to_datetime(d) for d in f] for f in foldls]\n",
    "df['fold'] = -1\n",
    "for t, fold in enumerate(foldls):\n",
    "    df['fold'][df.activation_date.isin(fold)] = t\n",
    "df['fold'].value_counts()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ridge_img'] = featrdgimg['ridge_img_preds'].values\n",
    "df = df.set_index('item_id')\n",
    "df.drop(['index'], axis=1,inplace=True)\n",
    "df.columns\n",
    "del featusrttl, featusrcat, featusrprd, featenc, featrdgprc, featimgprc, featnumf, featprmenc, featprmtro, featencfst, featprtfst, featldlag\n",
    "# del featusrttl, featusrcat, featusrprd, featenc, featrdgtxts\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[231.79067063331604] Feature Engineering\n",
      "Fill price\n",
      "Fill user_avg_price\n",
      "Fill usercat_avg_price\n",
      "Fill pcat_price_rratio\n",
      "Fill cat_price_rratio\n",
      "Fill ttl_price_rratio\n",
      "Fill dscr_price_rratio\n",
      "Fill pcat_log_price_rratio\n",
      "Fill user_log_price_rratio\n",
      "Fill cat_price_iratio\n",
      "Fill reg_price_iratio\n",
      "Fill reg_price_gratio\n",
      "Fill cty_price_gratio\n",
      "Fill ttlst_price_rratio\n",
      "Fill ttlst_city_price_rratio\n",
      "Fill ttlst_prm_price_rratio\n",
      "Fill par1cty_price_prratio\n",
      "Fill par2cty_price_prratio\n",
      "Fill par1utyp_price_prratio\n",
      "Fill par2utyp_price_prratio\n",
      "Fill rmean_price_byseq3_1\n",
      "Fill rmean_price_byseq3_2\n",
      "Fill rmean_price_byseq3_3\n",
      "Fill rmean_price_byseq8_1\n",
      "Fill rmean_price_byseq8_2\n",
      "Fill rmean_price_byseq8_3\n",
      "Fill price_datesort_lag\n",
      "Fill price_datesort_lead\n",
      "Fill price_seqsort_lag\n",
      "Fill price_seqsort_lead\n",
      "Fill price_min_sequence\n",
      "Fill price_min_sequence_diff\n",
      "fill user_activ_sum\n",
      "fill user_activ_mean\n",
      "fill user_activ_var\n",
      "[232.01624035835266] Manage Memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "user_id                                                                                        object\n",
       "region                                                                                         object\n",
       "city                                                                                           object\n",
       "parent_category_name                                                                           object\n",
       "category_name                                                                                  object\n",
       "param_1                                                                                        object\n",
       "param_2                                                                                        object\n",
       "param_3                                                                                        object\n",
       "title                                                                                          object\n",
       "description                                                                                    object\n",
       "price                                                                                         float32\n",
       "item_seq_number                                                                                 int32\n",
       "activation_date                                                                        datetime64[ns]\n",
       "user_type                                                                                      object\n",
       "image                                                                                          object\n",
       "image_top_1                                                                                   float32\n",
       "dullness                                                                                      float32\n",
       "whiteness                                                                                     float32\n",
       "dominant_red                                                                                  float32\n",
       "dominant_green                                                                                float32\n",
       "dominant_blue                                                                                 float32\n",
       "average_red                                                                                   float32\n",
       "average_green                                                                                 float32\n",
       "average_blue                                                                                  float32\n",
       "width                                                                                         float32\n",
       "height                                                                                        float32\n",
       "size                                                                                          float32\n",
       "blurness                                                                                      float32\n",
       "translation                                                                                    object\n",
       "population                                                                                    float32\n",
       "                                                                                            ...      \n",
       "parent_category_name_category_name_param_1_param_2_param_3_region_user_type_fratio5           float32\n",
       "parent_category_name_category_name_param_1_param_2_param_3_region_user_type_count5              int32\n",
       "na_ratio_user                                                                                 float32\n",
       "zero_ratio_user                                                                               float32\n",
       "rmean_price_byseq3_1                                                                          float32\n",
       "rmean_price_byseq3_2                                                                          float32\n",
       "rmean_price_byseq3_3                                                                          float32\n",
       "rmean_price_byseq8_1                                                                          float32\n",
       "rmean_price_byseq8_2                                                                          float32\n",
       "rmean_price_byseq8_3                                                                          float32\n",
       "price_datesort_lag                                                                            float32\n",
       "seq_datesort_lag                                                                                int32\n",
       "price_datesort_lead                                                                           float32\n",
       "seq_datesort_lead                                                                               int32\n",
       "seq_datesort_lag_diff                                                                           int32\n",
       "seq_datesort_lead_diff                                                                          int32\n",
       "price_seqsort_lag                                                                             float32\n",
       "seq_seqsort_lag                                                                                 int32\n",
       "price_seqsort_lead                                                                            float32\n",
       "seq_seqsort_lead                                                                                int32\n",
       "seq_seqsort_lag_diff                                                                            int32\n",
       "seq_seqsort_lead_diff                                                                           int32\n",
       "item_seq_repeat_postings                                                                        int32\n",
       "category_repeat_postings                                                                        int32\n",
       "price_min_sequence                                                                            float32\n",
       "price_min_sequence_diff                                                                       float32\n",
       "seq_min_sequence                                                                                int32\n",
       "seq_min_sequence_diff                                                                           int32\n",
       "fold                                                                                            int32\n",
       "ridge_img                                                                                     float32\n",
       "Length: 205, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('[{}] Feature Engineering'.format(time.time() - start_time))\n",
    "for col in df.columns:\n",
    "    if 'price' in col:\n",
    "        print(f'Fill {col}')\n",
    "        df[col].fillna(-999,inplace=True)\n",
    "\n",
    "for col in df.columns:\n",
    "    if 'user_activ' in col:\n",
    "        print(f'fill {col}')\n",
    "        df[col].fillna(-9,inplace=True)\n",
    "df[\"image_top_1\"].fillna(-999,inplace=True)\n",
    "\n",
    "del featct, featlatlon, featimgmeta, featpop, featrdgimg, featrdgtxt\n",
    "gc.collect()\n",
    "\n",
    "print('[{}] Manage Memory'.format(time.time() - start_time))\n",
    "for col in df.columns:\n",
    "    if np.float64 == df[col].dtype:\n",
    "        df[col] = df[col].astype(np.float32)\n",
    "    if np.int64 == df[col].dtype:\n",
    "        df[col] = df[col].astype(np.int32)\n",
    "    gc.collect()\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[303.16056632995605] Text Features\n",
      "[588.4742617607117] Text Features\n"
     ]
    }
   ],
   "source": [
    "print('[{}] Text Features'.format(time.time() - start_time))\n",
    "df['text_feat'] = df.apply(lambda row: ' '.join([\n",
    "    str(row['param_1']), \n",
    "    str(row['param_2']), \n",
    "    str(row['param_3'])]),axis=1) # Group Param Features\n",
    "df.drop([\"param_1\",\"param_2\",\"param_3\"],axis=1,inplace=True)\n",
    "\n",
    "print('[{}] Text Features'.format(time.time() - start_time))\n",
    "df['description'].fillna('unknowndescription', inplace=True)\n",
    "df['title'].fillna('unknowntitle', inplace=True)\n",
    "df['text']      = (df['description'].fillna('') + ' ' + df['title'] + ' ' + \n",
    "  df['parent_category_name'].fillna('').astype(str) + ' ' + df['category_name'].fillna('').astype(str) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[594.3163793087006] Create Time Variables\n",
      "[596.607809305191] Make Item Seq number as contiuous also\n",
      "[597.8697166442871] Encode Variables\n",
      "Encoding : ['region', 'parent_category_name', 'user_type', 'city', 'category_name', 'item_seq_number', 'image_top_1']\n"
     ]
    }
   ],
   "source": [
    "print('[{}] Create Time Variables'.format(time.time() - start_time))\n",
    "df[\"Weekday\"] = df['activation_date'].dt.weekday\n",
    "df.drop([\"activation_date\",\"image\"],axis=1,inplace=True)\n",
    "\n",
    "print('[{}] Make Item Seq number as contiuous also'.format(time.time() - start_time))\n",
    "df[\"item_seq_number_cont\"] = df[\"item_seq_number\"]\n",
    "df['city'] = df['region'].fillna('').astype(str) + '_' + df['city'].fillna('').astype(str)\n",
    "df.columns\n",
    "print('[{}] Encode Variables'.format(time.time() - start_time))\n",
    "df.drop(['user_id'], 1, inplace = True)\n",
    "categorical = [\"region\",\"parent_category_name\",\"user_type\", 'city', 'category_name', \"item_seq_number\", 'image_top_1']\n",
    "print(\"Encoding :\",categorical)\n",
    "# Encoder:\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for col in categorical:\n",
    "    df[col] = lbl.fit_transform(df[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[631.4115242958069] Meta Text Features\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2011862 entries, b912c3c6a6ad to d374d332992f\n",
      "Columns: 215 entries, region to title_words_vs_unique\n",
      "dtypes: float32(113), float64(3), int32(76), int64(17), object(6)\n",
      "memory usage: 1.8+ GB\n"
     ]
    }
   ],
   "source": [
    "print('[{}] Meta Text Features'.format(time.time() - start_time))\n",
    "textfeats = [\"description\",\"text_feat\", \"title\"]\n",
    "for cols in textfeats:\n",
    "    df[cols] = df[cols].astype(str) \n",
    "    df[cols] = df[cols].astype(str).fillna('nicapotato') # FILL NA\n",
    "    df[cols] = df[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently\n",
    "    df[cols + '_num_chars'] = df[cols].apply(len) # Count number of Characters\n",
    "    df[cols + '_num_words'] = df[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
    "    df[cols + '_num_unique_words'] = df[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df[cols + '_words_vs_unique'] = df[cols+'_num_unique_words'] / df[cols+'_num_words'] * 100 # Count Unique Words\n",
    "    gc.collect()\n",
    "df.info()\n",
    "for cols in ['translation']:\n",
    "    df[cols] = df[cols].astype(str) \n",
    "    df[cols] = df[cols].astype(str).fillna('nicapotato') # FILL NA\n",
    "    df[cols] = df[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[679.9395010471344] Manage Memory\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2011862 entries, b912c3c6a6ad to d374d332992f\n",
      "Columns: 215 entries, region to title_words_vs_unique\n",
      "dtypes: float32(116), int32(93), object(6)\n",
      "memory usage: 1.7+ GB\n"
     ]
    }
   ],
   "source": [
    "print('[{}] Manage Memory'.format(time.time() - start_time))\n",
    "for col in df.columns:\n",
    "    if np.float64 == df[col].dtype:\n",
    "        df[col] = df[col].astype(np.float32)\n",
    "    if np.int64 == df[col].dtype:\n",
    "        df[col] = df[col].astype(np.int32)\n",
    "    gc.collect()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[690.3659071922302] Clean text and tokenize\n",
      "description load tokenised [725.4076688289642]\n",
      "text load tokenised [727.5467128753662]\n",
      "text_feat load tokenised [729.7237927913666]\n",
      "title load tokenised [731.6259860992432]\n",
      "translation load tokenised [733.6010782718658]\n",
      "[735.8414347171783] Add some more test processing...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('[{}] Clean text and tokenize'.format(time.time() - start_time))\n",
    "toktok = ToktokTokenizer()\n",
    "tokSentMap = {}\n",
    "morpher = pymorphy2.MorphAnalyzer()\n",
    "def tokSent(sent):\n",
    "    sent = sent.replace('/', ' ')\n",
    "    return \" \".join(morpher.parse(word)[0].normal_form for word in toktok.tokenize(rgx.sub(' ', sent)))\n",
    "def tokCol(var):\n",
    "    return [tokSent(s) for s in var.tolist()]\n",
    "rgx = re.compile('[%s]' % '!\"#%&()*,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')   \n",
    "\n",
    "partitions = 4 \n",
    "def parallelize(data, func):\n",
    "    data_split = np.array_split(data.values, partitions)\n",
    "    pool = Pool(partitions)\n",
    "    data = pd.concat([pd.Series(l) for l in pool.map(tokCol, data_split)]).values\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data\n",
    "\n",
    "load_text = True\n",
    "text_cols = ['description', 'text', 'text_feat', 'title', 'translation']\n",
    "if load_text:\n",
    "    dftxt = pd.read_csv(path + '../features/text_features_morphed.csv.gz', compression = 'gzip')\n",
    "    for col in text_cols:\n",
    "        print(col + ' load tokenised [{}]'.format(time.time() - start_time))\n",
    "        df[col] = dftxt[col].values\n",
    "        df.fillna(' ', inplace = True)\n",
    "    del dftxt\n",
    "else:\n",
    "    for col in text_cols:\n",
    "        print(col + ' tokenise [{}]'.format(time.time() - start_time))\n",
    "        df[col] = parallelize(df[col], tokCol)\n",
    "    df[text_cols].to_csv(path + '../features/text_features_morphed.csv.gz', compression = 'gzip')\n",
    "gc.collect()\n",
    "\n",
    "print('[{}] Add some more test processing...'.format(time.time() - start_time))\n",
    "from itertools import combinations\n",
    "def create_bigrams(text):\n",
    "    try:\n",
    "        text = np.unique( [ w for w in text.split() ] )\n",
    "        lst_bi = []\n",
    "        for combo in combinations(text, 2):\n",
    "            cb1=combo[0]+combo[1]\n",
    "            cb2=combo[1]+combo[0]\n",
    "            in_dict=False\n",
    "            if cb1 in word_count_dict_one:\n",
    "                new_word = cb1\n",
    "                in_dict=True\n",
    "            if cb2 in word_count_dict_one:\n",
    "                new_word = cb2\n",
    "                in_dict=True\n",
    "            if not in_dict:\n",
    "                new_word = combo[0]+'___'+combo[1]\n",
    "            if len(cb1)>=0:\n",
    "                lst_bi.append(new_word)\n",
    "        return ' '.join( lst_bi )\n",
    "    except:\n",
    "        return ' '\n",
    "    \n",
    "def create_bigrams_df(df):\n",
    "    return df.apply( create_bigrams )\n",
    "def word_count(text, dc):\n",
    "    text = set( text.split(' ') ) \n",
    "    for w in text:\n",
    "        dc[w]+=1\n",
    "def remove_low_freq(text, dc):\n",
    "    return ' '.join( [w for w in text.split() if w in dc] )\n",
    "\n",
    "def parallelize_dataframe(df, func, cores = 4):\n",
    "    df_split = np.array_split(df, cores)\n",
    "    pool = Pool(cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#STORE ALL WORDS  FREQUENCY and Filter\n",
    "#################################################\n",
    "min_df_one=5\n",
    "min_df_bi=5\n",
    "df['text']      = (df['text'].fillna('') + ' ' + df['text_feat'].str.replace(' nan', ''))\n",
    "#df.drop('text_feat', 1, inplace = True )\n",
    "from collections import defaultdict\n",
    "df['name_bi'] = df['title'].fillna('')  + df['description'].apply( lambda x : ' '.join( x.split()[:5] ) )\n",
    "word_count_dict_one = defaultdict(np.uint32)\n",
    "df['name_bi'].apply(             lambda x : word_count(x, word_count_dict_one) )\n",
    "rare_words = [key for key in word_count_dict_one if  word_count_dict_one[key]<min_df_one ]\n",
    "for key in rare_words :\n",
    "    word_count_dict_one.pop(key, None)\n",
    "df['name_bi']      = df['name_bi'].apply( lambda x : remove_low_freq(x, word_count_dict_one) )\n",
    "word_count_dict_one=dict(word_count_dict_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_id\n",
      "b912c3c6a6ad    для___кокон малышдля для___пользоваться для___...\n",
      "2dac0150717d    вешалка___для одеждавешалка вешалка___под веша...\n",
      "ba83aefab5dc    philips___домашний philips___кинотеатр philips...\n",
      "02996f1dd2ea                           автокреслопродать___кресло\n",
      "7c90be56d2ab    2003весь___2110 2003весь___ваза 2003весь___воп...\n",
      "Name: name_bi, dtype: object\n",
      "[913.0737960338593] Finished CREATING BIGRAMS...\n",
      "[913.0741453170776] Finished tokenizing text...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>parent_category_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>item_seq_number</th>\n",
       "      <th>user_type</th>\n",
       "      <th>image_top_1</th>\n",
       "      <th>...</th>\n",
       "      <th>description_words_vs_unique</th>\n",
       "      <th>text_feat_num_chars</th>\n",
       "      <th>text_feat_num_words</th>\n",
       "      <th>text_feat_num_unique_words</th>\n",
       "      <th>text_feat_words_vs_unique</th>\n",
       "      <th>title_num_chars</th>\n",
       "      <th>title_num_words</th>\n",
       "      <th>title_num_unique_words</th>\n",
       "      <th>title_words_vs_unique</th>\n",
       "      <th>name_bi</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b912c3c6a6ad</th>\n",
       "      <td>19</td>\n",
       "      <td>1313</td>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>кокоби кокон для сон</td>\n",
       "      <td>кокон для сон малыш пользоваться маленький мес...</td>\n",
       "      <td>400.0</td>\n",
       "      <td>9108</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>для___кокон малышдля для___пользоваться для___...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2dac0150717d</th>\n",
       "      <td>17</td>\n",
       "      <td>1233</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>стойка для одежда</td>\n",
       "      <td>стойка для одежда под вешалка с бутик</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>8389</td>\n",
       "      <td>1</td>\n",
       "      <td>2723</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>66.666664</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>вешалка___для одеждавешалка вешалка___под веша...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ba83aefab5dc</th>\n",
       "      <td>16</td>\n",
       "      <td>1172</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>philips bluray</td>\n",
       "      <td>в хороший состояние домашний кинотеатр с blu r...</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>32856</td>\n",
       "      <td>1</td>\n",
       "      <td>2260</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>85.714287</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>philips___домашний philips___кинотеатр philips...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02996f1dd2ea</th>\n",
       "      <td>21</td>\n",
       "      <td>1557</td>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>автокресло</td>\n",
       "      <td>продать кресло от0 25кг</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>14637</td>\n",
       "      <td>0</td>\n",
       "      <td>2838</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>автокреслопродать___кресло</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7c90be56d2ab</th>\n",
       "      <td>4</td>\n",
       "      <td>250</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>ваза 2110 2003</td>\n",
       "      <td>весь вопрос по телефон</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>15299</td>\n",
       "      <td>1</td>\n",
       "      <td>1408</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2003весь___2110 2003весь___ваза 2003весь___воп...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              region  city  parent_category_name  category_name  \\\n",
       "item_id                                                           \n",
       "b912c3c6a6ad      19  1313                     4             42   \n",
       "2dac0150717d      17  1233                     2             22   \n",
       "ba83aefab5dc      16  1172                     0              2   \n",
       "02996f1dd2ea      21  1557                     4             42   \n",
       "7c90be56d2ab       4   250                     6              0   \n",
       "\n",
       "                             title  \\\n",
       "item_id                              \n",
       "b912c3c6a6ad  кокоби кокон для сон   \n",
       "2dac0150717d     стойка для одежда   \n",
       "ba83aefab5dc        philips bluray   \n",
       "02996f1dd2ea            автокресло   \n",
       "7c90be56d2ab        ваза 2110 2003   \n",
       "\n",
       "                                                    description    price  \\\n",
       "item_id                                                                    \n",
       "b912c3c6a6ad  кокон для сон малыш пользоваться маленький мес...    400.0   \n",
       "2dac0150717d              стойка для одежда под вешалка с бутик   3000.0   \n",
       "ba83aefab5dc  в хороший состояние домашний кинотеатр с blu r...   4000.0   \n",
       "02996f1dd2ea                            продать кресло от0 25кг   2200.0   \n",
       "7c90be56d2ab                             весь вопрос по телефон  40000.0   \n",
       "\n",
       "              item_seq_number  user_type  image_top_1  \\\n",
       "item_id                                                 \n",
       "b912c3c6a6ad             9108          1           13   \n",
       "2dac0150717d             8389          1         2723   \n",
       "ba83aefab5dc            32856          1         2260   \n",
       "02996f1dd2ea            14637          0         2838   \n",
       "7c90be56d2ab            15299          1         1408   \n",
       "\n",
       "                                    ...                          \\\n",
       "item_id                             ...                           \n",
       "b912c3c6a6ad                        ...                           \n",
       "2dac0150717d                        ...                           \n",
       "ba83aefab5dc                        ...                           \n",
       "02996f1dd2ea                        ...                           \n",
       "7c90be56d2ab                        ...                           \n",
       "\n",
       "              description_words_vs_unique  text_feat_num_chars  \\\n",
       "item_id                                                          \n",
       "b912c3c6a6ad                        100.0                   33   \n",
       "2dac0150717d                        100.0                   14   \n",
       "ba83aefab5dc                        100.0                   35   \n",
       "02996f1dd2ea                        100.0                   28   \n",
       "7c90be56d2ab                        100.0                   26   \n",
       "\n",
       "              text_feat_num_words  text_feat_num_unique_words  \\\n",
       "item_id                                                         \n",
       "b912c3c6a6ad                    4                           3   \n",
       "2dac0150717d                    3                           2   \n",
       "ba83aefab5dc                    7                           6   \n",
       "02996f1dd2ea                    4                           3   \n",
       "7c90be56d2ab                    5                           5   \n",
       "\n",
       "              text_feat_words_vs_unique  title_num_chars  title_num_words  \\\n",
       "item_id                                                                     \n",
       "b912c3c6a6ad                  75.000000               21                3   \n",
       "2dac0150717d                  66.666664               17                3   \n",
       "ba83aefab5dc                  85.714287               14                2   \n",
       "02996f1dd2ea                  75.000000               10                1   \n",
       "7c90be56d2ab                 100.000000               14                3   \n",
       "\n",
       "              title_num_unique_words  title_words_vs_unique  \\\n",
       "item_id                                                       \n",
       "b912c3c6a6ad                       3                  100.0   \n",
       "2dac0150717d                       3                  100.0   \n",
       "ba83aefab5dc                       2                  100.0   \n",
       "02996f1dd2ea                       1                  100.0   \n",
       "7c90be56d2ab                       3                  100.0   \n",
       "\n",
       "                                                        name_bi  \n",
       "item_id                                                          \n",
       "b912c3c6a6ad  для___кокон малышдля для___пользоваться для___...  \n",
       "2dac0150717d  вешалка___для одеждавешалка вешалка___под веша...  \n",
       "ba83aefab5dc  philips___домашний philips___кинотеатр philips...  \n",
       "02996f1dd2ea                         автокреслопродать___кресло  \n",
       "7c90be56d2ab  2003весь___2110 2003весь___ваза 2003весь___воп...  \n",
       "\n",
       "[5 rows x 216 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create ALL 2_ways combinations (Custom Bigrams)\n",
    "#################################################\n",
    "word_count_dict_bi=defaultdict(np.uint32)\n",
    "def word_count_bi(text):\n",
    "    text =  text.split(' ') \n",
    "    for w in text:\n",
    "        word_count_dict_bi[w]+=1\n",
    "\n",
    "df['name_bi']      = parallelize_dataframe( df['name_bi'],  create_bigrams_df )\n",
    "df['name_bi'].apply(word_count_bi )\n",
    "rare_words = [key for key in word_count_dict_bi if  word_count_dict_bi[key]<min_df_bi ]\n",
    "for key in rare_words :\n",
    "    word_count_dict_bi.pop(key, None)\n",
    "df['name_bi']      = df['name_bi'].apply( lambda x : remove_low_freq(x, word_count_dict_bi) )\n",
    "print(df['name_bi'].head())\n",
    "print('[{}] Finished CREATING BIGRAMS...'.format(time.time() - start_time))\n",
    "\n",
    "print('[{}] Finished tokenizing text...'.format(time.time() - start_time))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[913.2660965919495] [TF-IDF] Term Frequency Inverse Document Frequency Stage\n",
      "[2046.9955971240997] Vectorisation completed\n"
     ]
    }
   ],
   "source": [
    "print('[{}] [TF-IDF] Term Frequency Inverse Document Frequency Stage'.format(time.time() - start_time))\n",
    "russian_stop = set(stopwords.words('russian'))\n",
    "tfidf_para = {\n",
    "    \"stop_words\": russian_stop,\n",
    "    \"token_pattern\": r'\\w{1,}',\n",
    "    \"sublinear_tf\": True,\n",
    "    \"dtype\": np.float32,\n",
    "    \"smooth_idf\":False\n",
    "}\n",
    "countv_para = {\n",
    "    #\"stop_words\": russian_stop,\n",
    "    #\"analyzer\": 'word',\n",
    "    #\"token_pattern\": r'\\w{1,}',\n",
    "    \"lowercase\": True,\n",
    "    \"min_df\": 1 #False\n",
    "}\n",
    "def get_col(col_name): return lambda x: x[col_name]\n",
    "vectorizer = FeatureUnion([\n",
    "        ('text',TfidfVectorizer(\n",
    "            #ngram_range=(1, 2),\n",
    "            max_features=50000,\n",
    "            **tfidf_para,\n",
    "            preprocessor=get_col('text'))),\n",
    "        #('text_feat',CountVectorizer(\n",
    "        #    **countv_para,\n",
    "        #    preprocessor=get_col('text_feat'))),\n",
    "        ('name_bi',CountVectorizer(\n",
    "            **countv_para,\n",
    "            preprocessor=get_col('name_bi'))),\n",
    "        #('translation',TfidfVectorizer(\n",
    "        #    #ngram_range=(1, 2),\n",
    "        #    max_features=50000,\n",
    "        #    **tfidf_para,\n",
    "        #    preprocessor=get_col('translation'))),\n",
    "    ])\n",
    "    \n",
    "start_vect=time.time()\n",
    "vectorizer.fit(df.loc[traindex,:].to_dict('records'))\n",
    "ready_df = vectorizer.transform(df.to_dict('records'))\n",
    "tfvocab = vectorizer.get_feature_names()\n",
    "tfvocab[:50]\n",
    "print('[{}] Vectorisation completed'.format(time.time() - start_time))\n",
    "# Drop Text Cols\n",
    "df.drop(textfeats+['text', 'all_titles', 'translation', 'name_bi'], axis=1,inplace=True)\n",
    "#drop_cols= [c for c in textfeats+['text', 'all_titles', 'translation'] if c in df.columns]\n",
    "#df.drop(drop_cols, axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2048.4557497501373] Drop all the categorical\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "print('[{}] Drop all the categorical'.format(time.time() - start_time))\n",
    "df.drop(categorical, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sparseNN():                                             \n",
    "    sparse_data = Input( shape=[X_train.shape[1]], \n",
    "        dtype = 'float32',   sparse = True, name='sparse_data')  \n",
    "    \n",
    "    x = Dense(256 , kernel_initializer=he_uniform(seed=0) )(sparse_data)    \n",
    "    x = PReLU()(x)\n",
    "    x = Dense(256 , kernel_initializer=he_uniform(seed=0) )(x)\n",
    "    x = PReLU()(x)\n",
    "    x = Dense(64 , kernel_initializer=he_uniform(seed=0) )(x)\n",
    "    x = PReLU()(x)\n",
    "    x= Dense(1)(x)\n",
    "    \n",
    "    model = Model(sparse_data, x)\n",
    "    \n",
    "    optimizer = Adam()\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 [16707.806052207947] Modeling Stage\n",
      "Fold 0 : 1189868 Rows and 810429 Cols\n",
      "Fold 0 : 313556 Rows and 810429 Cols\n",
      "avg best iter: 0\n",
      "Train on 1189868 samples, validate on 313556 samples\n",
      "Epoch 1/1\n",
      "1189868/1189868 [==============================] - 571s 480us/step - loss: 0.0486 - val_loss: 0.0477\n",
      "RMSE fold 0 bag 0: 0.21851477733613833\n",
      "Train on 1189868 samples, validate on 313556 samples\n",
      "Epoch 1/1\n",
      "1189868/1189868 [==============================] - 572s 480us/step - loss: 0.0501 - val_loss: 0.0480\n",
      "RMSE fold 0 bag 1: 0.2179699513677337\n",
      "Train on 1189868 samples, validate on 313556 samples\n",
      "Epoch 1/1\n",
      "1189868/1189868 [==============================] - 1825s 2ms/step - loss: 0.0486 - val_loss: 0.0477\n",
      "RMSE fold 0 bag 2: 0.21756610496127282\n",
      "Model Evaluation Stage\n",
      "RMSE: 0.21756610496127282\n",
      "Fold 1 [19987.220673561096] Modeling Stage\n",
      "Fold 1 : 1176264 Rows and 810429 Cols\n",
      "Fold 1 : 327160 Rows and 810429 Cols\n",
      "avg best iter: 0\n",
      "Train on 1176264 samples, validate on 327160 samples\n",
      "Epoch 1/1\n",
      "1176264/1176264 [==============================] - 579s 492us/step - loss: 0.0489 - val_loss: 0.0480\n",
      "RMSE fold 1 bag 0: 0.21918120013794315\n",
      "Train on 1176264 samples, validate on 327160 samples\n",
      "Epoch 1/1\n",
      "1176264/1176264 [==============================] - 478s 407us/step - loss: 0.0483 - val_loss: 0.0479\n",
      "RMSE fold 1 bag 1: 0.21813988972199835\n",
      "Train on 1176264 samples, validate on 327160 samples\n",
      "Epoch 1/1\n",
      "1176264/1176264 [==============================] - 489s 416us/step - loss: 0.0486 - val_loss: 0.0479\n",
      "RMSE fold 1 bag 2: 0.21790330563339644\n",
      "Model Evaluation Stage\n",
      "RMSE: 0.21790330563339644\n",
      "Fold 2 [22070.604059934616] Modeling Stage\n",
      "Fold 2 : 1176532 Rows and 810429 Cols\n",
      "Fold 2 : 326892 Rows and 810429 Cols\n",
      "avg best iter: 0\n",
      "Train on 1176532 samples, validate on 326892 samples\n",
      "Epoch 1/1\n",
      "1176532/1176532 [==============================] - 564s 480us/step - loss: 0.0491 - val_loss: 0.0467\n",
      "RMSE fold 2 bag 0: 0.2160932769801051\n",
      "Train on 1176532 samples, validate on 326892 samples\n",
      "Epoch 1/1\n",
      "1176532/1176532 [==============================] - 560s 476us/step - loss: 0.0492 - val_loss: 0.0468\n",
      "RMSE fold 2 bag 1: 0.21535053838537094\n",
      "Train on 1176532 samples, validate on 326892 samples\n",
      "Epoch 1/1\n",
      "1176532/1176532 [==============================] - 2568s 2ms/step - loss: 0.0493 - val_loss: 0.0467\n",
      "RMSE fold 2 bag 2: 0.21498669948915106\n",
      "Model Evaluation Stage\n",
      "RMSE: 0.21498669948915106\n",
      "Fold 3 [26276.875683546066] Modeling Stage\n",
      "Fold 3 : 1195456 Rows and 810429 Cols\n",
      "Fold 3 : 307968 Rows and 810429 Cols\n",
      "avg best iter: 0\n",
      "Train on 1195456 samples, validate on 307968 samples\n",
      "Epoch 1/1\n",
      "1195456/1195456 [==============================] - 574s 480us/step - loss: 0.0485 - val_loss: 525198544000532032.0000\n",
      "RMSE fold 3 bag 0: 0.2194793451443252\n",
      "Train on 1195456 samples, validate on 307968 samples\n",
      "Epoch 1/1\n",
      "1195456/1195456 [==============================] - 490s 410us/step - loss: 0.0485 - val_loss: 33945656393842401280.0000\n",
      "RMSE fold 3 bag 1: 0.21919073374438688\n",
      "Train on 1195456 samples, validate on 307968 samples\n",
      "Epoch 1/1\n",
      "1195456/1195456 [==============================] - 491s 411us/step - loss: 0.0484 - val_loss: 12693899729749585920.0000\n",
      "RMSE fold 3 bag 2: 0.21846599637208328\n",
      "Model Evaluation Stage\n",
      "RMSE: 0.21846599637208328\n",
      "Fold 4 [28361.15731072426] Modeling Stage\n",
      "Fold 4 : 1275576 Rows and 810429 Cols\n",
      "Fold 4 : 227848 Rows and 810429 Cols\n",
      "avg best iter: 0\n",
      "Train on 1275576 samples, validate on 227848 samples\n",
      "Epoch 1/1\n",
      "1275576/1275576 [==============================] - 615s 482us/step - loss: 0.0486 - val_loss: 0.0467\n",
      "RMSE fold 4 bag 0: 0.21608660440469352\n",
      "Train on 1275576 samples, validate on 227848 samples\n",
      "Epoch 1/1\n",
      "1275576/1275576 [==============================] - 617s 484us/step - loss: 0.0490 - val_loss: 0.0467\n",
      "RMSE fold 4 bag 1: 0.21519639339784297\n",
      "Train on 1275576 samples, validate on 227848 samples\n",
      "Epoch 1/1\n",
      "1275576/1275576 [==============================] - 817s 641us/step - loss: 0.0487 - val_loss: 0.0466\n",
      "RMSE fold 4 bag 2: 0.21490427370502557\n",
      "Model Evaluation Stage\n",
      "RMSE: 0.21490427370502557\n",
      "Fold 5 [30965.306465625763] Modeling Stage\n",
      "Fold 5 : 1503424 Rows and 810429 Cols\n",
      "Fold 5 : 508438 Rows and 810429 Cols\n",
      "avg best iter: 0\n",
      "Epoch 1/1\n",
      "1503424/1503424 [==============================] - 753s 501us/step - loss: 0.0485\n",
      "Epoch 1/1\n",
      "1503424/1503424 [==============================] - 754s 501us/step - loss: 0.0483\n",
      "Epoch 1/1\n",
      "1503424/1503424 [==============================] - 756s 503us/step - loss: 0.0486\n",
      "Model Evaluation Stage\n"
     ]
    }
   ],
   "source": [
    "# Placeholder for predictions\n",
    "df['fold'].value_counts()\n",
    "y_pred_trn = pd.Series(-np.zeros(df.loc[traindex,:].shape[0]), index = traindex)\n",
    "y_pred_tst = pd.Series(-np.zeros(df.loc[testdex ,:].shape[0]), index = testdex)\n",
    "best_iters = []\n",
    "bags       = 1\n",
    "BATCH_SIZE = 5000\n",
    "nnbags     = 3\n",
    "bags       = 1\n",
    "for bag in range(bags):\n",
    "    for f in range(6):\n",
    "        print('Fold %s'%(f) + ' [{}] Modeling Stage'.format(time.time() - start_time))\n",
    "        trnidx = (df['fold'].loc[traindex] != f).values\n",
    "        trndf = df.drop('fold', 1).loc[traindex,:][trnidx].copy()\n",
    "        trndf[trndf>1000000] = 1000000\n",
    "        trndf[trndf<0] = 0\n",
    "        scaler = StandardScaler()\n",
    "        trndf = np.log1p(trndf.values)\n",
    "        trndf = scaler.fit_transform(trndf)\n",
    "        X_train = csr_matrix(hstack([csr_matrix(trndf),ready_df[0:traindex.shape[0]][trnidx]]))\n",
    "        y_train = y[trnidx]\n",
    "        # 5 is the test fold\n",
    "        if f == 5:\n",
    "            tstdf = df.drop('fold', 1).loc[testdex,:].copy()\n",
    "            tstdf[tstdf>1000000] = 1000000\n",
    "            tstdf[tstdf<-0] = 0\n",
    "            tstdf = np.log1p(tstdf.values)\n",
    "            tstdf = scaler.transform(tstdf)\n",
    "            X_test = csr_matrix(hstack([csr_matrix(tstdf),ready_df[traindex.shape[0]:]]))\n",
    "        else:\n",
    "            tstdf =  df.drop('fold', 1).loc[traindex,:][~trnidx].copy()\n",
    "            tstdf[tstdf>1000000] = 1000000\n",
    "            tstdf[tstdf<0] = 0\n",
    "            tstdf = np.log1p(tstdf.values)\n",
    "            tstdf = scaler.transform(tstdf)\n",
    "            X_test = csr_matrix(hstack([csr_matrix(tstdf), ready_df[0:traindex.shape[0]][~trnidx]]))\n",
    "            y_test  = y[~trnidx]\n",
    "        #tfvocab = df.drop('fold', 1).columns.tolist() + vectorizer.get_feature_names()\n",
    "        del trndf\n",
    "        gc.collect()\n",
    "        for shape in [X_train, X_test]:\n",
    "            print(\"Fold {} : {} Rows and {} Cols\".format(f, *shape.shape))\n",
    "        gc.collect();\n",
    "        gc.collect()\n",
    "    \n",
    "        if f==5:\n",
    "            best_iter = 0\n",
    "            print('avg best iter: %s'%(best_iter))\n",
    "            # ridge = Ridge(solver='auto', fit_intercept=True, alpha=0.4, max_iter=200, normalize=False, tol=0.01)\n",
    "            # ridge.fit(X_train, y_train)\n",
    "            y_predls = []\n",
    "            for ep in range(nnbags):\n",
    "                gc.collect()\n",
    "                sparse_nn = sparseNN()\n",
    "                sparse_nn.fit(X_train, y_train, \\\n",
    "                              batch_size=BATCH_SIZE, \\\n",
    "                              epochs=1, verbose=1 )\n",
    "                y_predls.append(np.clip(sparse_nn.predict(X_test, batch_size=20000), -0.1, 1.1))\n",
    "            y_pred = sum(y_predls)/len(y_predls)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            best_iter = 0\n",
    "            print('avg best iter: %s'%(best_iter))\n",
    "            # ridge = Ridge(solver='auto', fit_intercept=True, alpha=0.4, max_iter=200, normalize=False, tol=0.01)\n",
    "            # ridge.fit(X_train, y_train)\n",
    "            y_predls = []\n",
    "            for ep in range(nnbags):\n",
    "                gc.collect()\n",
    "                sparse_nn = sparseNN()\n",
    "                sparse_nn.fit(X_train, y_train, \\\n",
    "                              batch_size=BATCH_SIZE, \\\n",
    "                              validation_data = (X_test, y_test), \\\n",
    "                              epochs=1, verbose=1 )\n",
    "                y_predls.append(np.clip(sparse_nn.predict(X_test, batch_size=20000), -0.1, 1.1))\n",
    "                print('RMSE fold %s bag %s:'%(f, ep), np.sqrt(metrics.mean_squared_error(y_test, sum(y_predls)/len(y_predls))))\n",
    "            y_pred = sum(y_predls)/len(y_predls)\n",
    "            \n",
    "        print(\"Model Evaluation Stage\")\n",
    "        if f == 5:\n",
    "            y_pred_tst[:] += y_pred.flatten() # ridge.predict(X_test)\n",
    "        else:\n",
    "            y_pred_trn[~trnidx] += y_pred.flatten() # ridge.predict(X_test)\n",
    "            print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_trn[~trnidx])))\n",
    "        del X_test\n",
    "        gc.collect()\n",
    "        y_pred_trn.to_csv(\"rmlp5CV_2306A_trn.csv\",index=True)\n",
    "        y_pred_tst.to_csv(\"rmlp5CV_2306A_tst.csv\",index=True)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for all : 0.21686466841688262\n"
     ]
    }
   ],
   "source": [
    "lgsub = pd.concat([y_pred_trn, y_pred_tst]).reset_index()\n",
    "lgsub.rename(columns = {0 : 'deal_probability'}, inplace=True)\n",
    "lgsub['deal_probability'] = lgsub['deal_probability']/(bag+1)\n",
    "lgsub.set_index('item_id', inplace = True)\n",
    "print('RMSE for all :', np.sqrt(metrics.mean_squared_error(y, lgsub.loc[traindex])))\n",
    "lgsub.to_csv(\"rmlp5CV_2306A.csv.gz\",index=True,header=True, compression = 'gzip')\n",
    "\n",
    "lgsub.to_csv(path + \"../sub/rmlp5CV_2306A.csv.gz\",index=True,header=True, compression = 'gzip')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "f = 3\n",
    "print('Fold %s'%(f) + ' [{}] Modeling Stage'.format(time.time() - start_time))\n",
    "trnidx = (df['fold'].loc[traindex] != f).values\n",
    "trndf = df.drop('fold', 1).loc[traindex,:][trnidx].copy()\n",
    "tstdf =  df.drop('fold', 1).loc[traindex,:][~trnidx].copy()\n",
    "trndf[trndf>1000000] = 1000000\n",
    "trndf[trndf<0] = 0\n",
    "tstdf[tstdf>1000000] = 1000000\n",
    "tstdf[tstdf<0] = 0\n",
    "skew = trndf.skew()\n",
    "for col in trndf.columns:\n",
    "    if skew[col]>1:\n",
    "        trndf[col] = np.log1p(trndf[col].values)\n",
    "        tstdf[col] = np.log1p(tstdf[col].values)\n",
    "        scaler = StandardScaler()\n",
    "trndf = scaler.fit_transform(trndf)\n",
    "tstdf = scaler.transform(tstdf)\n",
    "X_train = csr_matrix(hstack([csr_matrix(trndf),ready_df[0:traindex.shape[0]][trnidx]]))\n",
    "y_train = y[trnidx]\n",
    "X_test = csr_matrix(hstack([csr_matrix(tstdf), ready_df[0:traindex.shape[0]][~trnidx]]))\n",
    "y_test  = y[~trnidx]\n",
    "gc.collect()\n",
    "sparse_nn = sparseNN()\n",
    "batch_size = 20000\n",
    "sparse_nn.fit(X_train, y_train, \\\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_data = (X_test, y_test), \n",
    "        epochs=1, verbose=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1195456 samples, validate on 307968 samples\n",
      "Epoch 1/1\n",
      "1195456/1195456 [==============================] - 574s 480us/step - loss: 0.0485 - val_loss: 0.0487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f34941d44a8>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.clip((sum(y_predls)/len(y_predls)), -0.1, 1.1).min()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
