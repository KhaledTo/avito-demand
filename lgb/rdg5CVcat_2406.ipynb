{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.723403930664062e-05] Load Train/Test\n",
      "Train shape: 1503424 Rows, 16 Columns\n",
      "Test shape: 508438 Rows, 16 Columns\n",
      "[27.67295241355896] Create Validation Index\n",
      "[27.68098783493042] Combine Train and Test\n",
      "\n",
      "All Data shape: 2011862 Rows, 17 Columns\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/tunguz/bow-meta-text-and-dense-features-lgbm-clone?scriptVersionId=3540839\n",
    "\n",
    "# Models Packages\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time, gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import pymorphy2\n",
    "import nltk, re\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from sklearn.linear_model import Ridge\n",
    "import itertools\n",
    "\n",
    "#path = '../input/'\n",
    "path = \"/home/darragh/avito/data/\"\n",
    "#path = '/Users/dhanley2/Documents/avito/data/'\n",
    "path = '/home/ubuntu/avito/data/'\n",
    "start_time = time.time()\n",
    "full = False\n",
    "\n",
    "print('[{}] Load Train/Test'.format(time.time() - start_time))\n",
    "traindf = pd.read_csv(path + 'train.csv.zip', index_col = \"item_id\", parse_dates = [\"activation_date\"], compression = 'zip')\n",
    "traindex = traindf.index\n",
    "testdf = pd.read_csv(path + 'test.csv.zip', index_col = \"item_id\", parse_dates = [\"activation_date\"])\n",
    "testdex = testdf.index\n",
    "y = traindf.deal_probability.copy()\n",
    "traindf.drop(\"deal_probability\",axis=1, inplace=True)\n",
    "print('Train shape: {} Rows, {} Columns'.format(*traindf.shape))\n",
    "print('Test shape: {} Rows, {} Columns'.format(*testdf.shape))\n",
    "traindf['activation_date'].value_counts()\n",
    "\n",
    "(traindf['image_top_1'] == traindf['image_top_1']).value_counts()\n",
    "(testdf['image_top_1'] == testdf['image_top_1']).value_counts()\n",
    "\n",
    "\n",
    "\n",
    "print('[{}] Create Validation Index'.format(time.time() - start_time))\n",
    "if full:\n",
    "    trnidx = (traindf.activation_date<=pd.to_datetime('2017-03-28')).values\n",
    "    validx = (traindf.activation_date>=pd.to_datetime('2017-03-29')).values\n",
    "else:\n",
    "    trnidx = (traindf.activation_date<=pd.to_datetime('2017-03-26')).values\n",
    "    validx = (traindf.activation_date>=pd.to_datetime('2017-03-27')).values\n",
    "\n",
    "print('[{}] Combine Train and Test'.format(time.time() - start_time))\n",
    "df = pd.concat([traindf,testdf],axis=0)\n",
    "del traindf,testdf\n",
    "gc.collect()\n",
    "df['idx'] = range(df.shape[0])\n",
    "print('\\nAll Data shape: {} Rows, {} Columns'.format(*df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item_id\n",
       "b912c3c6a6ad    4\n",
       "2dac0150717d    2\n",
       "ba83aefab5dc    0\n",
       "02996f1dd2ea    4\n",
       "7c90be56d2ab    6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl = preprocessing.LabelEncoder()\n",
    "pcatidx = lbl.fit_transform(df[\"parent_category_name\"].astype(str))\n",
    "pcatids = pd.Series(pcatidx, index=df.index)\n",
    "pcatids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77.09755635261536] Load meta image engineered features\n",
      "[96.90808320045471] Load translated image engineered features\n",
      "[125.78642749786377] Load other engineered features\n",
      "\n",
      "All Data shape: 2011862 Rows, 44 Columns\n"
     ]
    }
   ],
   "source": [
    "#print('[{}] Count NA row wise'.format(time.time() - start_time))\n",
    "#df['NA_count_rows'] = df.isnull().sum(axis=1)\n",
    "\n",
    "print('[{}] Load meta image engineered features'.format(time.time() - start_time))\n",
    "featimgmeta = pd.concat([pd.read_csv(path + '../features/img_features_%s.csv.gz'%(i)) for i in range(6)])\n",
    "featimgmeta.rename(columns = {'name':'image'}, inplace = True)\n",
    "featimgmeta['image'] = featimgmeta['image'].str.replace('.jpg', '')\n",
    "df = df.reset_index('item_id').merge(featimgmeta, on = ['image'], how = 'left').set_index('item_id')\n",
    "for col in featimgmeta.columns.values[1:]:\n",
    "    df[col].fillna(-1, inplace = True)\n",
    "    df[col].astype(np.float32, inplace = True)\n",
    "    \n",
    "print('[{}] Load translated image engineered features'.format(time.time() - start_time))\n",
    "feattrlten = pd.concat([pd.read_csv(path + '../features/translate_trn_en.csv.gz', compression = 'gzip'),\n",
    "                        pd.read_csv(path + '../features/translate_tst_en.csv.gz', compression = 'gzip')])\n",
    "# feattrlten = pd.concat([pd.read_pickle(path + '../features/translate_trn_en.pkl'),\n",
    "#                        pd.read_pickle(path + '../features/translate_tst_en.pkl')])\n",
    "feattrlten.fillna('', inplace = True)\n",
    "feattrlten['translation'] = feattrlten['title_translated'] + ' ' + feattrlten['param_1_translated'] + ' ' \\\n",
    "            + feattrlten['param_2_translated'] + ' ' + feattrlten['param_3_translated'] + ' '  \\\n",
    "            + feattrlten['category_name_translated'] + ' ' + feattrlten['parent_category_name_translated']\n",
    "feattrlten = feattrlten.set_index('item_id')[['translation']]\n",
    "feattrlten.head()\n",
    "df = pd.merge(df, feattrlten, left_index=True, right_index=True, how='left')\n",
    "del feattrlten\n",
    "gc.collect()\n",
    " \n",
    "print('[{}] Load other engineered features'.format(time.time() - start_time))\n",
    "featlatlon = pd.read_csv(path + '../features/avito_region_city_features.csv') # https://www.kaggle.com/frankherfert/region-and-city-details-with-lat-lon-and-clusters\n",
    "featlatlon.drop(['city_region', 'city_region_id', 'region_id'], 1, inplace = True)\n",
    "featpop    = pd.read_csv(path + '../features/city_population_wiki_v3.csv') # https://www.kaggle.com/stecasasso/russian-city-population-from-wikipedia/comments\n",
    "featusrttl = pd.read_csv(path + '../features/user_agg.csv.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featusrcat = pd.read_csv(path + '../features/usercat_agg.csv.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featusrprd = pd.read_csv(path + '../features/user_activ_period_stats.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featrdgtxt = pd.read_csv(path + '../features/ridgeText5CV.csv.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "#featrdgtxts = pd.read_csv(path + '../features/ridgeTextStr5CV.csv.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featrdgimg = pd.read_csv(path + '../features/ridgeImg5CV.csv.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "#featrdgprc = pd.read_csv(path + '../features/price_category_ratios.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featrdgprc = pd.read_csv(path + '../features/price_seq_category_ratios.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featrdgprc.fillna(-1, inplace = True)\n",
    "featrdgrnk = pd.read_csv(path + '../features/price_rank_ratios0906.gz', compression = 'gzip') # created with R script and stemmer\n",
    "featrdgrnk.isnull().sum()\n",
    "featnumf = pd.read_csv(path + '../features/numericFeats.gz', compression = 'gzip') \n",
    "featnumf.fillna(0, inplace = True)\n",
    "featencfst = pd.read_csv(path + '../features/alldf_bayes_fest_1206.gz', compression = 'gzip')\n",
    "featprtfst = pd.read_csv(path + '../features/pratios_fest_1206.gz', compression = 'gzip')\n",
    "\n",
    "featprmenc = pd.read_csv(path + '../features/alldf_bayes_mean_param_1006.gz', compression = 'gzip') \n",
    "featprmtro = pd.read_csv(path + '../features/price_param_ratios1006.gz', compression = 'gzip') \n",
    "#featimgnet = pd.read_csv(path + '../features/imgnet_decode_feats.csv.gz', compression = 'gzip')\n",
    "featldlag  = pd.read_csv(path + '../features/pseq_leadlag_festivities_1906.gz', compression = 'gzip')\n",
    "\n",
    "featimgprc = pd.read_csv(path + '../features/price_imagetop1_ratios.gz', compression = 'gzip') # created with features/make/priceImgRatios2705.R\n",
    "featenc = pd.read_csv(path + '../features/alldf_bayes_mean.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featct  = pd.read_csv(path + '../features/alldf_count.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featusrttl.rename(columns={'title': 'all_titles'}, inplace = True)\n",
    "df = df.reset_index().merge(featpop, on = 'city', how = 'left')\n",
    "df = df.merge(featlatlon, on = ['city', 'region'], how = 'left')\n",
    "df['population'].fillna(-1, inplace = True)\n",
    "df = df.set_index('item_id')\n",
    "keep = ['user_id', 'all_titles', 'user_avg_price', 'user_ad_ct']\n",
    "df = df.reset_index().merge(featusrttl[keep], on = 'user_id').set_index('item_id')\n",
    "keep = ['user_id', 'parent_category_name', 'usercat_avg_price', 'usercat_ad_ct']\n",
    "gc.collect()\n",
    "df = df.reset_index().merge(featusrcat[keep], on = ['user_id', 'parent_category_name']).set_index('item_id')\n",
    "keep = ['user_id', 'user_activ_sum', 'user_activ_mean', 'user_activ_var']\n",
    "gc.collect()\n",
    "df = df.reset_index().merge(featusrprd[keep], on = ['user_id'], how = 'left').set_index('item_id')\n",
    "#df = df.reset_index().merge(featimgnet, on = ['item_id'], how = 'left').set_index('item_id')\n",
    "\n",
    "print('\\nAll Data shape: {} Rows, {} Columns'.format(*df.shape))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[267.39500880241394] Resort data correctly\n",
      "[272.9590280056] Create folds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[283.25929284095764] Feature Engineering\n",
      "Fill price\n",
      "Fill user_avg_price\n",
      "Fill usercat_avg_price\n",
      "Fill pcat_price_rratio\n",
      "Fill cat_price_rratio\n",
      "Fill ttl_price_rratio\n",
      "Fill dscr_price_rratio\n",
      "Fill pcat_log_price_rratio\n",
      "Fill user_log_price_rratio\n",
      "Fill cat_price_iratio\n",
      "Fill reg_price_iratio\n",
      "Fill reg_price_gratio\n",
      "Fill cty_price_gratio\n",
      "Fill ttlst_price_rratio\n",
      "Fill ttlst_city_price_rratio\n",
      "Fill ttlst_prm_price_rratio\n",
      "Fill par1cty_price_prratio\n",
      "Fill par2cty_price_prratio\n",
      "Fill par1utyp_price_prratio\n",
      "Fill par2utyp_price_prratio\n",
      "Fill rmean_price_byseq3_1\n",
      "Fill rmean_price_byseq3_2\n",
      "Fill rmean_price_byseq3_3\n",
      "Fill rmean_price_byseq8_1\n",
      "Fill rmean_price_byseq8_2\n",
      "Fill rmean_price_byseq8_3\n",
      "Fill price_datesort_lag\n",
      "Fill price_datesort_lead\n",
      "Fill price_seqsort_lag\n",
      "Fill price_seqsort_lead\n",
      "Fill price_min_sequence\n",
      "Fill price_min_sequence_diff\n",
      "fill user_activ_sum\n",
      "fill user_activ_mean\n",
      "fill user_activ_var\n",
      "[283.4870054721832] Manage Memory\n",
      "[354.8749575614929] Text Features\n",
      "[637.7176022529602] Text Features\n",
      "[643.5269165039062] Create Time Variables\n",
      "[645.826756477356] Make Item Seq number as contiuous also\n",
      "[647.010502576828] Encode Variables\n",
      "Encoding : ['region', 'parent_category_name', 'user_type', 'city', 'category_name', 'item_seq_number', 'image_top_1']\n",
      "[679.8939535617828] Meta Text Features\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2011862 entries, b912c3c6a6ad to d374d332992f\n",
      "Columns: 215 entries, region to title_words_vs_unique\n",
      "dtypes: float32(113), float64(3), int32(76), int64(17), object(6)\n",
      "memory usage: 1.8+ GB\n",
      "[728.3251640796661] Manage Memory\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2011862 entries, b912c3c6a6ad to d374d332992f\n",
      "Columns: 215 entries, region to title_words_vs_unique\n",
      "dtypes: float32(116), int32(93), object(6)\n",
      "memory usage: 1.7+ GB\n",
      "[738.6553502082825] Clean text and tokenize\n",
      "description load tokenised [774.6118333339691]\n",
      "text load tokenised [776.7370045185089]\n",
      "text_feat load tokenised [778.9039862155914]\n",
      "title load tokenised [780.7892701625824]\n",
      "translation load tokenised [782.7741961479187]\n",
      "[785.0236215591431] Add some more test processing...\n",
      "item_id\n",
      "b912c3c6a6ad    для___кокон малышдля для___пользоваться для___...\n",
      "2dac0150717d    вешалка___для одеждавешалка вешалка___под веша...\n",
      "ba83aefab5dc    philips___домашний philips___кинотеатр philips...\n",
      "02996f1dd2ea                           автокреслопродать___кресло\n",
      "7c90be56d2ab    2003весь___2110 2003весь___ваза 2003весь___воп...\n",
      "Name: name_bi, dtype: object\n",
      "[964.1531841754913] Finished CREATING BIGRAMS...\n",
      "[964.1535506248474] Finished tokenizing text...\n",
      "[964.1546967029572] [TF-IDF] Term Frequency Inverse Document Frequency Stage\n",
      "[2127.1090795993805] Vectorisation completed\n"
     ]
    }
   ],
   "source": [
    "print('[{}] Resort data correctly'.format(time.time() - start_time))\n",
    "df.sort_values('idx', inplace = True)\n",
    "df.drop(['idx'], axis=1,inplace=True)\n",
    "df.reset_index(inplace = True)\n",
    "df.head()\n",
    "df = pd.concat([df.reset_index(),featenc, featct, featrdgtxt, featrdgprc, featimgprc, featrdgrnk, featnumf, featprmenc, featprmtro, featencfst, featprtfst, featldlag],axis=1)\n",
    "#df['ridge_txt'] = featrdgtxt['ridge_preds'].values\n",
    "#df = pd.concat([df.reset_index(),featenc, featct, ],axis=1)\n",
    "\n",
    "print('[{}] Create folds'.format(time.time() - start_time))\n",
    "foldls = [[\"2017-03-15\", \"2017-03-16\", \"2017-03-17\"], \\\n",
    "       [\"2017-03-18\", \"2017-03-19\", \"2017-03-20\"], \\\n",
    "       [\"2017-03-21\", \"2017-03-22\", \"2017-03-23\"], \\\n",
    "       [\"2017-03-24\", \"2017-03-25\", \"2017-03-26\"], \\\n",
    "        [\"2017-03-27\", \"2017-03-28\", \"2017-03-29\", \\\n",
    "            \"2017-03-30\", \"2017-03-31\", \"2017-04-01\", \\\n",
    "            \"2017-04-02\", \"2017-04-03\",\"2017-04-07\"]]\n",
    "foldls = [[pd.to_datetime(d) for d in f] for f in foldls]\n",
    "df['fold'] = -1\n",
    "for t, fold in enumerate(foldls):\n",
    "    df['fold'][df.activation_date.isin(fold)] = t\n",
    "df['fold'].value_counts()\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "df['ridge_img'] = featrdgimg['ridge_img_preds'].values\n",
    "df = df.set_index('item_id')\n",
    "df.drop(['index'], axis=1,inplace=True)\n",
    "df.columns\n",
    "del featusrttl, featusrcat, featusrprd, featenc, featrdgprc, featimgprc, featnumf, featprmenc, featprmtro, featencfst, featprtfst, featldlag\n",
    "# del featusrttl, featusrcat, featusrprd, featenc, featrdgtxts\n",
    "gc.collect()\n",
    "\n",
    "print('[{}] Feature Engineering'.format(time.time() - start_time))\n",
    "for col in df.columns:\n",
    "    if 'price' in col:\n",
    "        print(f'Fill {col}')\n",
    "        df[col].fillna(-999,inplace=True)\n",
    "\n",
    "for col in df.columns:\n",
    "    if 'user_activ' in col:\n",
    "        print(f'fill {col}')\n",
    "        df[col].fillna(-9,inplace=True)\n",
    "df[\"image_top_1\"].fillna(-999,inplace=True)\n",
    "\n",
    "del featct, featlatlon, featimgmeta, featpop, featrdgimg, featrdgtxt\n",
    "gc.collect()\n",
    "\n",
    "print('[{}] Manage Memory'.format(time.time() - start_time))\n",
    "for col in df.columns:\n",
    "    if np.float64 == df[col].dtype:\n",
    "        df[col] = df[col].astype(np.float32)\n",
    "    if np.int64 == df[col].dtype:\n",
    "        df[col] = df[col].astype(np.int32)\n",
    "    gc.collect()\n",
    "df.dtypes\n",
    "\n",
    "\n",
    "print('[{}] Text Features'.format(time.time() - start_time))\n",
    "df['text_feat'] = df.apply(lambda row: ' '.join([\n",
    "    str(row['param_1']), \n",
    "    str(row['param_2']), \n",
    "    str(row['param_3'])]),axis=1) # Group Param Features\n",
    "df.drop([\"param_1\",\"param_2\",\"param_3\"],axis=1,inplace=True)\n",
    "\n",
    "print('[{}] Text Features'.format(time.time() - start_time))\n",
    "df['description'].fillna('unknowndescription', inplace=True)\n",
    "df['title'].fillna('unknowntitle', inplace=True)\n",
    "df['text']      = (df['description'].fillna('') + ' ' + df['title'] + ' ' + \n",
    "  df['parent_category_name'].fillna('').astype(str) + ' ' + df['category_name'].fillna('').astype(str) )\n",
    "\n",
    "\n",
    "\n",
    "print('[{}] Create Time Variables'.format(time.time() - start_time))\n",
    "df[\"Weekday\"] = df['activation_date'].dt.weekday\n",
    "df.drop([\"activation_date\",\"image\"],axis=1,inplace=True)\n",
    "\n",
    "print('[{}] Make Item Seq number as contiuous also'.format(time.time() - start_time))\n",
    "df[\"item_seq_number_cont\"] = df[\"item_seq_number\"]\n",
    "df['city'] = df['region'].fillna('').astype(str) + '_' + df['city'].fillna('').astype(str)\n",
    "df.columns\n",
    "print('[{}] Encode Variables'.format(time.time() - start_time))\n",
    "df.drop(['user_id'], 1, inplace = True)\n",
    "categorical = [\"region\",\"parent_category_name\",\"user_type\", 'city', 'category_name', \"item_seq_number\", 'image_top_1']\n",
    "print(\"Encoding :\",categorical)\n",
    "# Encoder:\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for col in categorical:\n",
    "    df[col] = lbl.fit_transform(df[col].astype(str))\n",
    "  \n",
    "print('[{}] Meta Text Features'.format(time.time() - start_time))\n",
    "textfeats = [\"description\",\"text_feat\", \"title\"]\n",
    "for cols in textfeats:\n",
    "    df[cols] = df[cols].astype(str) \n",
    "    df[cols] = df[cols].astype(str).fillna('nicapotato') # FILL NA\n",
    "    df[cols] = df[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently\n",
    "    df[cols + '_num_chars'] = df[cols].apply(len) # Count number of Characters\n",
    "    df[cols + '_num_words'] = df[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
    "    df[cols + '_num_unique_words'] = df[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df[cols + '_words_vs_unique'] = df[cols+'_num_unique_words'] / df[cols+'_num_words'] * 100 # Count Unique Words\n",
    "    gc.collect()\n",
    "df.info()\n",
    "for cols in ['translation']:\n",
    "    df[cols] = df[cols].astype(str) \n",
    "    df[cols] = df[cols].astype(str).fillna('nicapotato') # FILL NA\n",
    "    df[cols] = df[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently\n",
    "\n",
    "    \n",
    "print('[{}] Manage Memory'.format(time.time() - start_time))\n",
    "for col in df.columns:\n",
    "    if np.float64 == df[col].dtype:\n",
    "        df[col] = df[col].astype(np.float32)\n",
    "    if np.int64 == df[col].dtype:\n",
    "        df[col] = df[col].astype(np.int32)\n",
    "    gc.collect()\n",
    "df.info()\n",
    "\n",
    "\n",
    "print('[{}] Clean text and tokenize'.format(time.time() - start_time))\n",
    "toktok = ToktokTokenizer()\n",
    "tokSentMap = {}\n",
    "morpher = pymorphy2.MorphAnalyzer()\n",
    "def tokSent(sent):\n",
    "    sent = sent.replace('/', ' ')\n",
    "    return \" \".join(morpher.parse(word)[0].normal_form for word in toktok.tokenize(rgx.sub(' ', sent)))\n",
    "def tokCol(var):\n",
    "    return [tokSent(s) for s in var.tolist()]\n",
    "rgx = re.compile('[%s]' % '!\"#%&()*,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')   \n",
    "\n",
    "partitions = 4 \n",
    "def parallelize(data, func):\n",
    "    data_split = np.array_split(data.values, partitions)\n",
    "    pool = Pool(partitions)\n",
    "    data = pd.concat([pd.Series(l) for l in pool.map(tokCol, data_split)]).values\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data\n",
    "\n",
    "load_text = True\n",
    "text_cols = ['description', 'text', 'text_feat', 'title', 'translation']\n",
    "if load_text:\n",
    "    dftxt = pd.read_csv(path + '../features/text_features_morphed.csv.gz', compression = 'gzip')\n",
    "    for col in text_cols:\n",
    "        print(col + ' load tokenised [{}]'.format(time.time() - start_time))\n",
    "        df[col] = dftxt[col].values\n",
    "        df.fillna(' ', inplace = True)\n",
    "    del dftxt\n",
    "else:\n",
    "    for col in text_cols:\n",
    "        print(col + ' tokenise [{}]'.format(time.time() - start_time))\n",
    "        df[col] = parallelize(df[col], tokCol)\n",
    "    df[text_cols].to_csv(path + '../features/text_features_morphed.csv.gz', compression = 'gzip')\n",
    "gc.collect()\n",
    "\n",
    "print('[{}] Add some more test processing...'.format(time.time() - start_time))\n",
    "from itertools import combinations\n",
    "def create_bigrams(text):\n",
    "    try:\n",
    "        text = np.unique( [ w for w in text.split() ] )\n",
    "        lst_bi = []\n",
    "        for combo in combinations(text, 2):\n",
    "            cb1=combo[0]+combo[1]\n",
    "            cb2=combo[1]+combo[0]\n",
    "            in_dict=False\n",
    "            if cb1 in word_count_dict_one:\n",
    "                new_word = cb1\n",
    "                in_dict=True\n",
    "            if cb2 in word_count_dict_one:\n",
    "                new_word = cb2\n",
    "                in_dict=True\n",
    "            if not in_dict:\n",
    "                new_word = combo[0]+'___'+combo[1]\n",
    "            if len(cb1)>=0:\n",
    "                lst_bi.append(new_word)\n",
    "        return ' '.join( lst_bi )\n",
    "    except:\n",
    "        return ' '\n",
    "    \n",
    "def create_bigrams_df(df):\n",
    "    return df.apply( create_bigrams )\n",
    "def word_count(text, dc):\n",
    "    text = set( text.split(' ') ) \n",
    "    for w in text:\n",
    "        dc[w]+=1\n",
    "def remove_low_freq(text, dc):\n",
    "    return ' '.join( [w for w in text.split() if w in dc] )\n",
    "\n",
    "def parallelize_dataframe(df, func, cores = 4):\n",
    "    df_split = np.array_split(df, cores)\n",
    "    pool = Pool(cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "#STORE ALL WORDS  FREQUENCY and Filter\n",
    "#################################################\n",
    "min_df_one=5\n",
    "min_df_bi=5\n",
    "df['text']      = (df['text'].fillna('') + ' ' + df['text_feat'].str.replace(' nan', ''))\n",
    "#df.drop('text_feat', 1, inplace = True )\n",
    "from collections import defaultdict\n",
    "df['name_bi'] = df['title'].fillna('')  + df['description'].apply( lambda x : ' '.join( x.split()[:5] ) )\n",
    "word_count_dict_one = defaultdict(np.uint32)\n",
    "df['name_bi'].apply(             lambda x : word_count(x, word_count_dict_one) )\n",
    "rare_words = [key for key in word_count_dict_one if  word_count_dict_one[key]<min_df_one ]\n",
    "for key in rare_words :\n",
    "    word_count_dict_one.pop(key, None)\n",
    "df['name_bi']      = df['name_bi'].apply( lambda x : remove_low_freq(x, word_count_dict_one) )\n",
    "word_count_dict_one=dict(word_count_dict_one)\n",
    "\n",
    "#Create ALL 2_ways combinations (Custom Bigrams)\n",
    "#################################################\n",
    "word_count_dict_bi=defaultdict(np.uint32)\n",
    "def word_count_bi(text):\n",
    "    text =  text.split(' ') \n",
    "    for w in text:\n",
    "        word_count_dict_bi[w]+=1\n",
    "\n",
    "df['name_bi']      = parallelize_dataframe( df['name_bi'],  create_bigrams_df )\n",
    "df['name_bi'].apply(word_count_bi )\n",
    "rare_words = [key for key in word_count_dict_bi if  word_count_dict_bi[key]<min_df_bi ]\n",
    "for key in rare_words :\n",
    "    word_count_dict_bi.pop(key, None)\n",
    "df['name_bi']      = df['name_bi'].apply( lambda x : remove_low_freq(x, word_count_dict_bi) )\n",
    "print(df['name_bi'].head())\n",
    "print('[{}] Finished CREATING BIGRAMS...'.format(time.time() - start_time))\n",
    "\n",
    "print('[{}] Finished tokenizing text...'.format(time.time() - start_time))\n",
    "df.head()\n",
    "print('[{}] [TF-IDF] Term Frequency Inverse Document Frequency Stage'.format(time.time() - start_time))\n",
    "russian_stop = set(stopwords.words('russian'))\n",
    "tfidf_para = {\n",
    "    \"stop_words\": russian_stop,\n",
    "    \"token_pattern\": r'\\w{1,}',\n",
    "    \"sublinear_tf\": True,\n",
    "    \"dtype\": np.float32,\n",
    "    \"smooth_idf\":False\n",
    "}\n",
    "countv_para = {\n",
    "    #\"stop_words\": russian_stop,\n",
    "    #\"analyzer\": 'word',\n",
    "    #\"token_pattern\": r'\\w{1,}',\n",
    "    \"lowercase\": True,\n",
    "    \"min_df\": 1 #False\n",
    "}\n",
    "def get_col(col_name): return lambda x: x[col_name]\n",
    "vectorizer = FeatureUnion([\n",
    "        ('text',TfidfVectorizer(\n",
    "            #ngram_range=(1, 2),\n",
    "            max_features=50000,\n",
    "            **tfidf_para,\n",
    "            preprocessor=get_col('text'))),\n",
    "        ('name_bi',CountVectorizer(\n",
    "            **countv_para,\n",
    "            preprocessor=get_col('name_bi'))),\n",
    "    ])\n",
    "    \n",
    "start_vect=time.time()\n",
    "vectorizer.fit(df.loc[traindex,:].to_dict('records'))\n",
    "ready_df = vectorizer.transform(df.to_dict('records'))\n",
    "tfvocab = vectorizer.get_feature_names()\n",
    "tfvocab[:50]\n",
    "print('[{}] Vectorisation completed'.format(time.time() - start_time))\n",
    "# Drop Text Cols\n",
    "df.drop(textfeats+['text', 'all_titles', 'translation', 'name_bi'], axis=1,inplace=True)\n",
    "#drop_cols= [c for c in textfeats+['text', 'all_titles', 'translation'] if c in df.columns]\n",
    "#df.drop(drop_cols, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2128.5873432159424] Drop all the categorical\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "print('[{}] Drop all the categorical'.format(time.time() - start_time))\n",
    "df.drop(categorical, axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 [6348.859169721603] Modeling Stage\n"
     ]
    }
   ],
   "source": [
    "# Placeholder for predictions\n",
    "df['fold'].value_counts()\n",
    "y_pred_trn = pd.Series(-np.zeros(df.loc[traindex,:].shape[0]), index = traindex)\n",
    "y_pred_tst = pd.Series(-np.zeros(df.loc[testdex ,:].shape[0]), index = testdex)\n",
    "best_iters = []\n",
    "bags       = 1\n",
    "for bag in range(bags):\n",
    "    lgbm_params['seed'] = bag+1\n",
    "    for f in range(6):\n",
    "        print('Fold %s'%(f) + ' [{}] Modeling Stage'.format(time.time() - start_time))\n",
    "        trnidx = (df['fold'].loc[traindex] != f).values\n",
    "        trndf = df.drop('fold', 1).loc[traindex,:][trnidx].copy()\n",
    "        trndf[trndf>10000] = 10000\n",
    "        trndf[trndf<0] = 0\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        trndf = scaler.fit_transform(trndf.values)\n",
    "        X_train = csr_matrix(hstack([csr_matrix(trndf),ready_df[0:traindex.shape[0]][trnidx]]))\n",
    "        # X_train = hstack([csr_matrix(np.log1p(0.43526 + df.drop('fold', 1).loc[traindex,:][trnidx].values)),ready_df[0:traindex.shape[0]][trnidx]])\n",
    "        y_train = y[trnidx]\n",
    "        pcattrn = pcatids.loc[traindex][trnidx].values\n",
    "        # 5 is the test fold\n",
    "        if f == 5:\n",
    "            tstdf = df.drop('fold', 1).loc[testdex,:].copy()\n",
    "            tstdf[tstdf>10000] = 10000\n",
    "            tstdf[tstdf<-0] = 0\n",
    "            tstdf = scaler.transform(tstdf.values)\n",
    "            X_test = csr_matrix( hstack([csr_matrix(tstdf),ready_df[traindex.shape[0]:]]))\n",
    "            pcattst = pcatids.loc[testdex].values\n",
    "            # X_test = hstack([csr_matrix(np.log1p(0.43526 + df.drop('fold', 1).loc[testdex,:].values)),ready_df[traindex.shape[0]:]])\n",
    "        else:\n",
    "            tstdf =  df.drop('fold', 1).loc[traindex,:][~trnidx].copy()\n",
    "            tstdf[tstdf>10000] = 10000\n",
    "            tstdf[tstdf<0] = 0\n",
    "            tstdf = scaler.transform(tstdf.values)\n",
    "            X_test = csr_matrix(hstack([csr_matrix(tstdf), ready_df[0:traindex.shape[0]][~trnidx]]))\n",
    "            pcattst = pcatids.loc[traindex][~trnidx].values\n",
    "            # X_test = hstack([csr_matrix(np.log1p(0.43526 + df.drop('fold', 1).loc[traindex,:][~trnidx].values)),ready_df[0:traindex.shape[0]][~trnidx]])\n",
    "            y_test  = y[~trnidx]\n",
    "        tfvocab = df.drop('fold', 1).columns.tolist() + vectorizer.get_feature_names()\n",
    "        del trndf\n",
    "        gc.collect()\n",
    "        for shape in [X_train, X_test]:\n",
    "            print(\"Fold {} : {} Rows and {} Cols\".format(f, *shape.shape))\n",
    "        gc.collect();\n",
    "        gc.collect()\n",
    "    \n",
    "        predls = []\n",
    "    \n",
    "        if f==5:\n",
    "            best_iter = 0# int(np.mean(best_iters))\n",
    "\n",
    "            for p in range(9):\n",
    "                print('Fold %s Pcat %s'%(f, p) + ' [{}]'.format(time.time() - start_time))\n",
    "                ridge = Ridge(solver='auto', fit_intercept=True, alpha=0.4, max_iter=200, normalize=False, tol=0.01)\n",
    "                ridge.fit(X_train[pcattrn==p], y_train[pcattrn==p])\n",
    "                predls.append(ridge.predict(X_test[pcattst==p]))\n",
    "        else:\n",
    "            best_iter = 0# int(np.mean(best_iters))\n",
    "            y_pred_trntmp = np.zeros(y_pred_trn[~trnidx].shape[0])\n",
    "            for p in range(9):\n",
    "                \n",
    "                print('Fold %s Pcat %s'%(f, p) + ' [{}]'.format(time.time() - start_time))\n",
    "                ridge = Ridge(solver='auto', fit_intercept=True, alpha=0.4, max_iter=200, normalize=False, tol=0.01)\n",
    "                ridge.fit(X_train[pcattrn==p], y_train[pcattrn==p])\n",
    "                predls.append( ridge.predict(X_test[pcattst==p]))\n",
    "                #print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test[pcattst==p], y_pred_trn[~trnidx][pcattst==p])))\n",
    "            #print('RMSE Full Fold:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_trn[~trnidx])))\n",
    "        \n",
    "        pred = list(itertools.chain(*predls))\n",
    "        print(\"Model Evaluation Stage\")\n",
    "        if f == 5:\n",
    "            y_pred_tst[:] += pred # ridge.predict(X_test)\n",
    "        else:\n",
    "            y_pred_trn[~trnidx] += pred #ridge.predict(X_test)\n",
    "            print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_trn[~trnidx])))\n",
    "        del X_test, X_train\n",
    "        gc.collect()\n",
    "        y_pred_trn.to_csv(\"rdg5CV_2406_trn.csv\",index=True)\n",
    "        y_pred_tst.to_csv(\"rdg5CV_2406_tst.csv\",index=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(10).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "list2d = [[1,2,3],[4,5,6], [7], [8,9]]\n",
    "list(itertools.chain(*list2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4    914200\n",
    "#2    243733\n",
    "#0    231290\n",
    "#5    210577\n",
    "#8    117282\n",
    "#6    109792\n",
    "#7     88004\n",
    "#3     72446\n",
    "#1     24538\n",
    "#dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for all : 0.2949544182370951\n"
     ]
    }
   ],
   "source": [
    "lgsub = pd.concat([y_pred_trn, y_pred_tst]).reset_index()\n",
    "lgsub.rename(columns = {0 : 'deal_probability'}, inplace=True)\n",
    "lgsub['deal_probability'] = lgsub['deal_probability']/(bag+1)\n",
    "lgsub.set_index('item_id', inplace = True)\n",
    "print('RMSE for all :', np.sqrt(metrics.mean_squared_error(y, lgsub.loc[traindex])))\n",
    "# RMSE for all : 0.2168\n",
    "lgsub.to_csv(\"rdg5CV_2406.csv.gz\",index=True,header=True, compression = 'gzip')\n",
    "\n",
    "lgsub.to_csv(path + \"../sub/rdg5CV_2406.csv.gz\",index=True,header=True, compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for all : 0.2949072612236387\n"
     ]
    }
   ],
   "source": [
    "print('RMSE for all :', np.sqrt(metrics.mean_squared_error(y, np.clip(lgsub.loc[traindex].values,0.0001, 0.9999))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deal_probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b912c3c6a6ad</th>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2dac0150717d</th>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ba83aefab5dc</th>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02996f1dd2ea</th>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7c90be56d2ab</th>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              deal_probability\n",
       "item_id                       \n",
       "b912c3c6a6ad              -0.0\n",
       "2dac0150717d              -0.0\n",
       "ba83aefab5dc              -0.0\n",
       "02996f1dd2ea              -0.0\n",
       "7c90be56d2ab              -0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgsub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
