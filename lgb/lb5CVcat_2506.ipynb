{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/tunguz/bow-meta-text-and-dense-features-lgbm-clone?scriptVersionId=3540839\n",
    "\n",
    "# Models Packages\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time, gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import pymorphy2\n",
    "import nltk, re\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from multiprocessing import cpu_count, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.365776062011719e-05] Load Train/Test\n",
      "Train shape: 1503424 Rows, 16 Columns\n",
      "Test shape: 508438 Rows, 16 Columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2017-03-20    115190\n",
       "2017-03-27    114863\n",
       "2017-03-19    114416\n",
       "2017-03-26    113513\n",
       "2017-03-28    112885\n",
       "2017-03-21    110535\n",
       "2017-03-22    109813\n",
       "2017-03-15    108615\n",
       "2017-03-23    106544\n",
       "2017-03-16    106168\n",
       "2017-03-17     98773\n",
       "2017-03-18     97554\n",
       "2017-03-24     97351\n",
       "2017-03-25     97104\n",
       "2017-03-29        87\n",
       "2017-04-01         3\n",
       "2017-03-30         3\n",
       "2017-04-02         3\n",
       "2017-04-03         2\n",
       "2017-04-07         1\n",
       "2017-03-31         1\n",
       "Name: activation_date, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#path = '../input/'\n",
    "path = \"/home/darragh/avito/data/\"\n",
    "path = '/Users/dhanley2/Documents/avito/data/'\n",
    "path = '/home/ubuntu/avito/data/'\n",
    "start_time = time.time()\n",
    "full = False\n",
    "\n",
    "print('[{}] Load Train/Test'.format(time.time() - start_time))\n",
    "traindf = pd.read_csv(path + 'train.csv.zip', index_col = \"item_id\", parse_dates = [\"activation_date\"], compression = 'zip')\n",
    "traindex = traindf.index\n",
    "testdf = pd.read_csv(path + 'test.csv.zip', index_col = \"item_id\", parse_dates = [\"activation_date\"])\n",
    "testdex = testdf.index\n",
    "y = traindf.deal_probability.copy()\n",
    "traindf.drop(\"deal_probability\",axis=1, inplace=True)\n",
    "print('Train shape: {} Rows, {} Columns'.format(*traindf.shape))\n",
    "print('Test shape: {} Rows, {} Columns'.format(*testdf.shape))\n",
    "traindf['activation_date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27.34650421142578] Create Validation Index\n",
      "[27.353004693984985] Combine Train and Test\n",
      "\n",
      "All Data shape: 2011862 Rows, 17 Columns\n"
     ]
    }
   ],
   "source": [
    "print('[{}] Create Validation Index'.format(time.time() - start_time))\n",
    "if full:\n",
    "    trnidx = (traindf.activation_date<=pd.to_datetime('2017-03-28')).values\n",
    "    validx = (traindf.activation_date>=pd.to_datetime('2017-03-29')).values\n",
    "else:\n",
    "    trnidx = (traindf.activation_date<=pd.to_datetime('2017-03-26')).values\n",
    "    validx = (traindf.activation_date>=pd.to_datetime('2017-03-27')).values\n",
    "\n",
    "print('[{}] Combine Train and Test'.format(time.time() - start_time))\n",
    "df = pd.concat([traindf,testdf],axis=0)\n",
    "del traindf,testdf\n",
    "gc.collect()\n",
    "df['idx'] = range(df.shape[0])\n",
    "print('\\nAll Data shape: {} Rows, {} Columns'.format(*df.shape))\n",
    "\n",
    "#print('[{}] Count NA row wise'.format(time.time() - start_time))\n",
    "#df['NA_count_rows'] = df.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "item_id\n",
       "b912c3c6a6ad     7\n",
       "2dac0150717d     2\n",
       "ba83aefab5dc     0\n",
       "02996f1dd2ea     7\n",
       "7c90be56d2ab    10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcats = ((df[\"parent_category_name\"]))\n",
    "cats  = (df[\"parent_category_name\"]+'  ---   '+df[\"category_name\"])\n",
    "pcats[df[\"parent_category_name\" ]=='Личные вещи'] = cats[df[\"parent_category_name\" ]=='Личные вещи']\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "pcatidx = lbl.fit_transform((pcats).astype(str))\n",
    "pcatids = pd.Series(pcatidx, index=df.index)\n",
    "pcatids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36.71019387245178] Load meta image engineered features\n",
      "[55.74893045425415] Load translated image engineered features\n",
      "[84.28807520866394] Load other engineered features\n",
      "\n",
      "All Data shape: 1097662 Rows, 44 Columns\n"
     ]
    }
   ],
   "source": [
    "print('[{}] Load meta image engineered features'.format(time.time() - start_time))\n",
    "featimgmeta = pd.concat([pd.read_csv(path + '../features/img_features_%s.csv.gz'%(i)) for i in range(6)])\n",
    "featimgmeta.rename(columns = {'name':'image'}, inplace = True)\n",
    "featimgmeta['image'] = featimgmeta['image'].str.replace('.jpg', '')\n",
    "df = df.reset_index('item_id').merge(featimgmeta, on = ['image'], how = 'left').set_index('item_id')\n",
    "for col in featimgmeta.columns.values[1:]:\n",
    "    df[col].fillna(-1, inplace = True)\n",
    "    df[col].astype(np.float32, inplace = True)\n",
    "    \n",
    "print('[{}] Load translated image engineered features'.format(time.time() - start_time))\n",
    "feattrlten = pd.concat([pd.read_csv(path + '../features/translate_trn_en.csv.gz', compression = 'gzip'),\n",
    "                        pd.read_csv(path + '../features/translate_tst_en.csv.gz', compression = 'gzip')])\n",
    "# feattrlten = pd.concat([pd.read_pickle(path + '../features/translate_trn_en.pkl'),\n",
    "#                        pd.read_pickle(path + '../features/translate_tst_en.pkl')])\n",
    "feattrlten.fillna('', inplace = True)\n",
    "feattrlten['translation'] = feattrlten['title_translated'] + ' ' + feattrlten['param_1_translated'] + ' ' \\\n",
    "            + feattrlten['param_2_translated'] + ' ' + feattrlten['param_3_translated'] + ' '  \\\n",
    "            + feattrlten['category_name_translated'] + ' ' + feattrlten['parent_category_name_translated']\n",
    "feattrlten = feattrlten.set_index('item_id')[['translation']]\n",
    "feattrlten.head()\n",
    "df = pd.merge(df, feattrlten, left_index=True, right_index=True, how='left')\n",
    "del feattrlten\n",
    "gc.collect()\n",
    " \n",
    "print('[{}] Load other engineered features'.format(time.time() - start_time))\n",
    "featlatlon = pd.read_csv(path + '../features/avito_region_city_features.csv') # https://www.kaggle.com/frankherfert/region-and-city-details-with-lat-lon-and-clusters\n",
    "featlatlon.drop(['city_region', 'city_region_id', 'region_id'], 1, inplace = True)\n",
    "featpop    = pd.read_csv(path + '../features/city_population_wiki_v3.csv') # https://www.kaggle.com/stecasasso/russian-city-population-from-wikipedia/comments\n",
    "featusrttl = pd.read_csv(path + '../features/user_agg.csv.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featusrcat = pd.read_csv(path + '../features/usercat_agg.csv.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featusrprd = pd.read_csv(path + '../features/user_activ_period_stats.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featrdgtxt = pd.read_csv(path + '../features/ridgeText5CV.csv.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "#featrdgtxts = pd.read_csv(path + '../features/ridgeTextStr5CV.csv.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featrdgimg = pd.read_csv(path + '../features/ridgeImg5CV.csv.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "#featrdgprc = pd.read_csv(path + '../features/price_category_ratios.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featrdgprc = pd.read_csv(path + '../features/price_seq_category_ratios.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featrdgprc.fillna(-1, inplace = True)\n",
    "featrdgrnk = pd.read_csv(path + '../features/price_rank_ratios0906.gz', compression = 'gzip') # created with R script and stemmer\n",
    "featrdgrnk.isnull().sum()\n",
    "featnumf = pd.read_csv(path + '../features/numericFeats.gz', compression = 'gzip') \n",
    "featnumf.fillna(0, inplace = True)\n",
    "featencfst = pd.read_csv(path + '../features/alldf_bayes_fest_1206.gz', compression = 'gzip')\n",
    "featprtfst = pd.read_csv(path + '../features/pratios_fest_1206.gz', compression = 'gzip')\n",
    "\n",
    "featprmenc = pd.read_csv(path + '../features/alldf_bayes_mean_param_1006.gz', compression = 'gzip') \n",
    "featprmtro = pd.read_csv(path + '../features/price_param_ratios1006.gz', compression = 'gzip') \n",
    "\n",
    "featimgprc = pd.read_csv(path + '../features/price_imagetop1_ratios.gz', compression = 'gzip') # created with features/make/priceImgRatios2705.R\n",
    "featenc = pd.read_csv(path + '../features/alldf_bayes_mean.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featct  = pd.read_csv(path + '../features/alldf_count.gz', compression = 'gzip') # created with features/make/user_actagg_1705.py\n",
    "featusrttl.rename(columns={'title': 'all_titles'}, inplace = True)\n",
    "df = df.reset_index().merge(featpop, on = 'city', how = 'left')\n",
    "df = df.merge(featlatlon, on = ['city', 'region'], how = 'left')\n",
    "df['population'].fillna(-1, inplace = True)\n",
    "df = df.set_index('item_id')\n",
    "keep = ['user_id', 'all_titles', 'user_avg_price', 'user_ad_ct']\n",
    "df = df.reset_index().merge(featusrttl[keep], on = 'user_id').set_index('item_id')\n",
    "keep = ['user_id', 'parent_category_name', 'usercat_avg_price', 'usercat_ad_ct']\n",
    "gc.collect()\n",
    "df = df.reset_index().merge(featusrcat[keep], on = ['user_id', 'parent_category_name']).set_index('item_id')\n",
    "keep = ['user_id', 'user_activ_sum', 'user_activ_mean', 'user_activ_var']\n",
    "gc.collect()\n",
    "df = df.reset_index().merge(featusrprd[keep], on = ['user_id'], how = 'left').set_index('item_id')\n",
    "print('\\nAll Data shape: {} Rows, {} Columns'.format(*df.shape))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[208.81043076515198] Resort data correctly\n",
      "[215.16724610328674] Create folds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[224.25359654426575] Feature Engineering\n",
      "Fill price\n",
      "Fill user_avg_price\n",
      "Fill usercat_avg_price\n",
      "Fill pcat_price_rratio\n",
      "Fill cat_price_rratio\n",
      "Fill ttl_price_rratio\n",
      "Fill dscr_price_rratio\n",
      "Fill pcat_log_price_rratio\n",
      "Fill user_log_price_rratio\n",
      "Fill cat_price_iratio\n",
      "Fill reg_price_iratio\n",
      "Fill reg_price_gratio\n",
      "Fill cty_price_gratio\n",
      "Fill ttlst_price_rratio\n",
      "Fill ttlst_city_price_rratio\n",
      "Fill ttlst_prm_price_rratio\n",
      "Fill par1cty_price_prratio\n",
      "Fill par2cty_price_prratio\n",
      "Fill par1utyp_price_prratio\n",
      "Fill par2utyp_price_prratio\n",
      "fill user_activ_sum\n",
      "fill user_activ_mean\n",
      "fill user_activ_var\n",
      "[224.45432114601135] Manage Memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "user_id                                                                                                   object\n",
       "region                                                                                                    object\n",
       "city                                                                                                      object\n",
       "parent_category_name                                                                                      object\n",
       "category_name                                                                                             object\n",
       "param_1                                                                                                   object\n",
       "param_2                                                                                                   object\n",
       "param_3                                                                                                   object\n",
       "title                                                                                                     object\n",
       "description                                                                                               object\n",
       "price                                                                                                    float32\n",
       "item_seq_number                                                                                          float32\n",
       "activation_date                                                                                   datetime64[ns]\n",
       "user_type                                                                                                 object\n",
       "image                                                                                                     object\n",
       "image_top_1                                                                                              float32\n",
       "dullness                                                                                                 float32\n",
       "whiteness                                                                                                float32\n",
       "dominant_red                                                                                             float32\n",
       "dominant_green                                                                                           float32\n",
       "dominant_blue                                                                                            float32\n",
       "average_red                                                                                              float32\n",
       "average_green                                                                                            float32\n",
       "average_blue                                                                                             float32\n",
       "width                                                                                                    float32\n",
       "height                                                                                                   float32\n",
       "size                                                                                                     float32\n",
       "blurness                                                                                                 float32\n",
       "translation                                                                                               object\n",
       "population                                                                                               float32\n",
       "                                                                                                       ...      \n",
       "parent_category_name_category_name_param_1_param_2_param_3_region_city_title_fratio5                     float32\n",
       "parent_category_name_category_name_param_1_param_2_param_3_region_city_title_count5                        int32\n",
       "parent_category_name_category_name_param_1_param_2_param_3_region_title_fratio5                          float32\n",
       "parent_category_name_category_name_param_1_param_2_param_3_region_title_count5                             int32\n",
       "parent_category_name_region_title_fratio5                                                                float32\n",
       "parent_category_name_region_title_count5                                                                   int32\n",
       "parent_category_name_category_name_param_1_param_2_param_3_region_fratio5                                float32\n",
       "parent_category_name_category_name_param_1_param_2_param_3_region_count5                                   int32\n",
       "parent_category_name_category_name_param_1_param_2_user_id_fratio5                                       float32\n",
       "parent_category_name_category_name_param_1_param_2_user_id_count5                                          int32\n",
       "parent_category_name_category_name_user_id_fratio5                                                       float32\n",
       "parent_category_name_category_name_user_id_count5                                                          int32\n",
       "parent_category_name_category_name_param_1_user_id_fratio5                                               float32\n",
       "parent_category_name_category_name_param_1_user_id_count5                                                  int32\n",
       "parent_category_name_category_name_param_1_user_id_city_fratio5                                          float32\n",
       "parent_category_name_category_name_param_1_user_id_city_count5                                             int32\n",
       "parent_category_name_category_name_param_1_param_2_param_3_region_user_id_city_fratio5                   float32\n",
       "parent_category_name_category_name_param_1_param_2_param_3_region_user_id_city_count5                      int32\n",
       "parent_category_name_category_name_param_1_param_2_param_3_region_user_id_fratio5                        float32\n",
       "parent_category_name_category_name_param_1_param_2_param_3_region_user_id_count5                           int32\n",
       "parent_category_name_category_name_param_1_param_2_param_3_region_city_user_type_fratio5                 float32\n",
       "parent_category_name_category_name_param_1_param_2_param_3_region_city_user_type_count5                    int32\n",
       "parent_category_name_category_name_param_1_param_2_param_3_region_city_title_user_type_fratio5           float32\n",
       "parent_category_name_category_name_param_1_param_2_param_3_region_city_title_user_type_count5              int32\n",
       "parent_category_name_category_name_param_1_param_2_param_3_region_title_user_type_fratio5                float32\n",
       "parent_category_name_category_name_param_1_param_2_param_3_region_title_user_type_count5                   int32\n",
       "parent_category_name_category_name_param_1_param_2_param_3_region_user_type_fratio5                      float32\n",
       "parent_category_name_category_name_param_1_param_2_param_3_region_user_type_count5                         int32\n",
       "fold                                                                                                       int32\n",
       "ridge_img                                                                                                float32\n",
       "Length: 179, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('[{}] Resort data correctly'.format(time.time() - start_time))\n",
    "df.sort_values('idx', inplace = True)\n",
    "df.drop(['idx'], axis=1,inplace=True)\n",
    "df.reset_index(inplace = True)\n",
    "df.head()\n",
    "df = pd.concat([df.reset_index(),featenc, featct, featrdgtxt, featrdgprc, featimgprc, featrdgrnk, featnumf, featprmenc, featprmtro, featencfst, featprtfst],axis=1)\n",
    "#df['ridge_txt'] = featrdgtxt['ridge_preds'].values\n",
    "#df = pd.concat([df.reset_index(),featenc, featct, ],axis=1)\n",
    "\n",
    "print('[{}] Create folds'.format(time.time() - start_time))\n",
    "foldls = [[\"2017-03-15\", \"2017-03-16\", \"2017-03-17\"], \\\n",
    "       [\"2017-03-18\", \"2017-03-19\", \"2017-03-20\"], \\\n",
    "       [\"2017-03-21\", \"2017-03-22\", \"2017-03-23\"], \\\n",
    "       [\"2017-03-24\", \"2017-03-25\", \"2017-03-26\"], \\\n",
    "        [\"2017-03-27\", \"2017-03-28\", \"2017-03-29\", \\\n",
    "            \"2017-03-30\", \"2017-03-31\", \"2017-04-01\", \\\n",
    "            \"2017-04-02\", \"2017-04-03\",\"2017-04-07\"]]\n",
    "foldls = [[pd.to_datetime(d) for d in f] for f in foldls]\n",
    "df['fold'] = -1\n",
    "for t, fold in enumerate(foldls):\n",
    "    df['fold'][df.activation_date.isin(fold)] = t\n",
    "df['fold'].value_counts()\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "df['ridge_img'] = featrdgimg['ridge_img_preds'].values\n",
    "df = df.set_index('item_id')\n",
    "df.drop(['index'], axis=1,inplace=True)\n",
    "df.columns\n",
    "del featusrttl, featusrcat, featusrprd, featenc, featrdgprc, featimgprc, featnumf, featprmenc, featprmtro, featencfst, featprtfst\n",
    "# del featusrttl, featusrcat, featusrprd, featenc, featrdgtxts\n",
    "gc.collect()\n",
    "\n",
    "print('[{}] Feature Engineering'.format(time.time() - start_time))\n",
    "for col in df.columns:\n",
    "    if 'price' in col:\n",
    "        print(f'Fill {col}')\n",
    "        df[col].fillna(-999,inplace=True)\n",
    "\n",
    "for col in df.columns:\n",
    "    if 'user_activ' in col:\n",
    "        print(f'fill {col}')\n",
    "        df[col].fillna(-9,inplace=True)\n",
    "df[\"image_top_1\"].fillna(-999,inplace=True)\n",
    "\n",
    "del featct, featlatlon, featimgmeta, featpop, featrdgimg, featrdgtxt\n",
    "gc.collect()\n",
    "\n",
    "print('[{}] Manage Memory'.format(time.time() - start_time))\n",
    "for col in df.columns:\n",
    "    if np.float64 == df[col].dtype:\n",
    "        df[col] = df[col].astype(np.float32)\n",
    "    if np.int64 == df[col].dtype:\n",
    "        df[col] = df[col].astype(np.int32)\n",
    "    gc.collect()\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[283.33135747909546] Text Features\n"
     ]
    }
   ],
   "source": [
    "print('[{}] Text Features'.format(time.time() - start_time))\n",
    "df['text_feat'] = df.apply(lambda row: ' '.join([\n",
    "    str(row['param_1']), \n",
    "    str(row['param_2']), \n",
    "    str(row['param_3'])]),axis=1) # Group Param Features\n",
    "df.drop([\"param_1\",\"param_2\",\"param_3\"],axis=1,inplace=True)\n",
    "\n",
    "print('[{}] Text Features'.format(time.time() - start_time))\n",
    "df['description'].fillna('unknowndescription', inplace=True)\n",
    "df['title'].fillna('unknowntitle', inplace=True)\n",
    "df['text']      = (df['description'].fillna('') + ' ' + df['title'] + ' ' + \n",
    "  df['parent_category_name'].fillna('').astype(str) + ' ' + df['category_name'].fillna('').astype(str) )\n",
    "\n",
    "print('[{}] Create Time Variables'.format(time.time() - start_time))\n",
    "df[\"Weekday\"] = df['activation_date'].dt.weekday\n",
    "df.drop([\"activation_date\",\"image\"],axis=1,inplace=True)\n",
    "\n",
    "print('[{}] Make Item Seq number as contiuous also'.format(time.time() - start_time))\n",
    "df[\"item_seq_number_cont\"] = df[\"item_seq_number\"]\n",
    "df['city'] = df['region'].fillna('').astype(str) + '_' + df['city'].fillna('').astype(str)\n",
    "df.columns\n",
    "print('[{}] Encode Variables'.format(time.time() - start_time))\n",
    "df.drop(['user_id'], 1, inplace = True)\n",
    "categorical = [\"region\",\"parent_category_name\",\"user_type\", 'city', 'category_name', \"item_seq_number\", 'image_top_1']\n",
    "print(\"Encoding :\",categorical)\n",
    "# Encoder:\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for col in categorical:\n",
    "    df[col] = lbl.fit_transform(df[col].astype(str))\n",
    "  \n",
    "print('[{}] Meta Text Features'.format(time.time() - start_time))\n",
    "textfeats = [\"description\",\"text_feat\", \"title\"]\n",
    "for cols in textfeats:\n",
    "    df[cols] = df[cols].astype(str) \n",
    "    df[cols] = df[cols].astype(str).fillna('nicapotato') # FILL NA\n",
    "    df[cols] = df[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently\n",
    "    df[cols + '_num_chars'] = df[cols].apply(len) # Count number of Characters\n",
    "    df[cols + '_num_words'] = df[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
    "    df[cols + '_num_unique_words'] = df[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df[cols + '_words_vs_unique'] = df[cols+'_num_unique_words'] / df[cols+'_num_words'] * 100 # Count Unique Words\n",
    "    gc.collect()\n",
    "df.info()\n",
    "for cols in ['translation']:\n",
    "    df[cols] = df[cols].astype(str) \n",
    "    df[cols] = df[cols].astype(str).fillna('nicapotato') # FILL NA\n",
    "    df[cols] = df[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently\n",
    "\n",
    "    \n",
    "print('[{}] Manage Memory'.format(time.time() - start_time))\n",
    "for col in df.columns:\n",
    "    if np.float64 == df[col].dtype:\n",
    "        df[col] = df[col].astype(np.float32)\n",
    "    if np.int64 == df[col].dtype:\n",
    "        df[col] = df[col].astype(np.int32)\n",
    "    gc.collect()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[{}] Clean text and tokenize'.format(time.time() - start_time))\n",
    "toktok = ToktokTokenizer()\n",
    "tokSentMap = {}\n",
    "morpher = pymorphy2.MorphAnalyzer()\n",
    "def tokSent(sent):\n",
    "    sent = sent.replace('/', ' ')\n",
    "    return \" \".join(morpher.parse(word)[0].normal_form for word in toktok.tokenize(rgx.sub(' ', sent)))\n",
    "def tokCol(var):\n",
    "    return [tokSent(s) for s in var.tolist()]\n",
    "rgx = re.compile('[%s]' % '!\"#%&()*,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')   \n",
    "\n",
    "partitions = 4 \n",
    "def parallelize(data, func):\n",
    "    data_split = np.array_split(data.values, partitions)\n",
    "    pool = Pool(partitions)\n",
    "    data = pd.concat([pd.Series(l) for l in pool.map(tokCol, data_split)]).values\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data\n",
    "\n",
    "load_text = True\n",
    "text_cols = ['description', 'text', 'text_feat', 'title', 'translation']\n",
    "if load_text:\n",
    "    dftxt = pd.read_csv(path + '../features/text_features_morphed.csv.gz', compression = 'gzip')\n",
    "    for col in text_cols:\n",
    "        print(col + ' load tokenised [{}]'.format(time.time() - start_time))\n",
    "        df[col] = dftxt[col].values\n",
    "        df.fillna(' ', inplace = True)\n",
    "    del dftxt\n",
    "else:\n",
    "    for col in text_cols:\n",
    "        print(col + ' tokenise [{}]'.format(time.time() - start_time))\n",
    "        df[col] = parallelize(df[col], tokCol)\n",
    "    df[text_cols].to_csv(path + '../features/text_features_morphed.csv.gz', compression = 'gzip')\n",
    "gc.collect()\n",
    "\n",
    "print('[{}] Finished tokenizing text...'.format(time.time() - start_time))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[{}] [TF-IDF] Term Frequency Inverse Document Frequency Stage'.format(time.time() - start_time))\n",
    "russian_stop = set(stopwords.words('russian'))\n",
    "tfidf_para = {\n",
    "    \"stop_words\": russian_stop,\n",
    "    \"token_pattern\": r'\\w{1,}',\n",
    "    \"sublinear_tf\": True,\n",
    "    \"dtype\": np.float32,\n",
    "    \"smooth_idf\":False\n",
    "}\n",
    "countv_para = {\n",
    "    \"stop_words\": russian_stop,\n",
    "    \"analyzer\": 'word',\n",
    "    \"token_pattern\": r'\\w{1,}',\n",
    "    \"lowercase\": True,\n",
    "    \"min_df\": 10\n",
    "}\n",
    "def get_col(col_name): return lambda x: x[col_name]\n",
    "vectorizer = FeatureUnion([\n",
    "        ('text',TfidfVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=50000,\n",
    "            **tfidf_para,\n",
    "            preprocessor=get_col('text'))),\n",
    "        ('text_feat',CountVectorizer(\n",
    "            **countv_para,\n",
    "            preprocessor=get_col('text_feat'))),\n",
    "        ('title',CountVectorizer(\n",
    "            **countv_para,\n",
    "            preprocessor=get_col('title'))),\n",
    "    ])\n",
    "    \n",
    "start_vect=time.time()\n",
    "vectorizer.fit(df.loc[traindex,:].to_dict('records'))\n",
    "ready_df = vectorizer.transform(df.to_dict('records'))\n",
    "tfvocab = vectorizer.get_feature_names()\n",
    "tfvocab[:50]\n",
    "print('[{}] Vectorisation completed'.format(time.time() - start_time))\n",
    "# Drop Text Cols\n",
    "df.drop(textfeats+['text', 'all_titles', 'translation'], axis=1,inplace=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[{}] Drop all the categorical'.format(time.time() - start_time))\n",
    "df.drop(categorical, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training and Validation Set\n",
    "lgbm_params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective' : 'regression',\n",
    "    'metric' : 'rmse',\n",
    "    'num_leaves' : 500,\n",
    "    'nthread': 16,\n",
    "    'learning_rate' : 0.02,\n",
    "    'feature_fraction' : 0.5,\n",
    "    'verbosity' : 0,\n",
    "    'seed'      : 11\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pcatids = pd.Series(pcatidx, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for predictions\n",
    "df['fold'].value_counts()\n",
    "y_pred_trn = pd.Series(-np.zeros(df.loc[traindex,:].shape[0]), index = traindex)\n",
    "y_pred_tst = pd.Series(-np.zeros(df.loc[testdex ,:].shape[0]), index = testdex)\n",
    "\n",
    "for f in range(6):\n",
    "    print('Fold %s'%(f) + ' [{}] Modeling Stage'.format(time.time() - start_time))\n",
    "    trnidx = (df['fold'].loc[traindex] != f).values\n",
    "    X_train = csr_matrix(hstack([csr_matrix(df.drop('fold', 1).loc[traindex,:][trnidx].values),ready_df[0:traindex.shape[0]][trnidx]]))\n",
    "    y_train = y[trnidx]\n",
    "\n",
    "    pcattrn = pcatids.loc[traindex][trnidx].values\n",
    "    # 5 is the test fold\n",
    "    if f == 5:\n",
    "        X_test = csr_matrix(hstack([csr_matrix(df.drop('fold', 1).loc[testdex,:].values),ready_df[traindex.shape[0]:]]))\n",
    "        pcattst = pcatids.loc[testdex].values\n",
    "    else:\n",
    "        X_test = csr_matrix(hstack([csr_matrix(df.drop('fold', 1).loc[traindex,:][~trnidx].values),ready_df[0:traindex.shape[0]][~trnidx]]))\n",
    "        pcattst = pcatids.loc[traindex][~trnidx].values\n",
    "        y_test  = y[~trnidx]\n",
    "    tfvocab = df.drop('fold', 1).columns.tolist() + vectorizer.get_feature_names()\n",
    "    for shape in [X_train, X_test]:\n",
    "        print(\"Fold {} : {} Rows and {} Cols\".format(f, *shape.shape))\n",
    "    gc.collect();\n",
    "    pred = np.zeros(pcattst.shape[0])\n",
    "    \n",
    "    # Run a model on everiy parent category\n",
    "    for p in range(9):\n",
    "        # LGBM Dataset Formatting \n",
    "        print('Fold %s Pcat %s'%(f, p) + ' [{}]'.format(time.time() - start_time))\n",
    "\n",
    "        lgtrain = lgb.Dataset(X_train[pcattrn==p], y_train[pcattrn==p],\n",
    "                            feature_name=tfvocab)\n",
    "        lgb_clf = lgb.train(\n",
    "            lgbm_params,\n",
    "            lgtrain,    \n",
    "            num_boost_round = 900,\n",
    "            verbose_eval=2000)   \n",
    "        pred[pcattst==p] = lgb_clf.predict(X_test[pcattst==p])\n",
    "        del lgtrain\n",
    "        gc.collect()\n",
    "        if f != 5:\n",
    "            print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test[pcattst==p], pred[pcattst==p])))\n",
    "        \n",
    "    print(\"Model Evaluation Stage\")\n",
    "    if f == 5:\n",
    "        y_pred_tst[:] = pred\n",
    "    else:\n",
    "        y_pred_trn[~trnidx] = pred\n",
    "        print('RMSE Full Fold:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_trn[~trnidx])))\n",
    "        \n",
    "    ## LGBM Dataset Formatting \n",
    "    #lgtrain = lgb.Dataset(X_train, y_train,\n",
    "    #                feature_name=tfvocab)\n",
    "    del X_train, y_train\n",
    "    gc.collect()\n",
    "\n",
    "    #lgb_clf = lgb.train(\n",
    "    #    lgbm_params,\n",
    "    #    lgtrain,    \n",
    "    #    num_boost_round = 1200,\n",
    "    #    verbose_eval=200)    \n",
    "\n",
    "    #print(\"Model Evaluation Stage\")\n",
    "    #if f == 5:\n",
    "    #    y_pred_tst[:] = lgb_clf.predict(X_test)\n",
    "    #else:\n",
    "    #    y_pred_trn[~trnidx] = lgb_clf.predict(X_test)\n",
    "    #    print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_trn[~trnidx])))\n",
    "    del X_test\n",
    "    gc.collect()\n",
    "    y_pred_trn.to_csv(\"lgbCV_2506_trn.csv\",index=True)\n",
    "    y_pred_tst.to_csv(\"lgbCV_2506_tst.csv\",index=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4    914200\n",
    "#2    243733\n",
    "#0    231290\n",
    "#5    210577\n",
    "#8    117282\n",
    "#6    109792\n",
    "#7     88004\n",
    "#3     72446\n",
    "#1     24538"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgsub = pd.concat([y_pred_trn, y_pred_tst]).reset_index()\n",
    "lgsub.rename(columns = {0 : 'deal_probability'}, inplace=True)\n",
    "#lgsub['deal_probability'].clip(0.0, 1.0, inplace=True)\n",
    "lgsub.set_index('item_id', inplace = True)\n",
    "print('RMSE for all :', np.sqrt(metrics.mean_squared_error(y, lgsub.loc[traindex])))\n",
    "# RMSE for all : 0.2168\n",
    "lgsub.to_csv(\"lgCV_2506.csv.gz\",index=True,header=True, compression = 'gzip')\n",
    "\n",
    "lgsub.to_csv(path + \"../sub/lgCV_2506.csv.gz\",index=True,header=True, compression = 'gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
