{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darragh/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning:\n",
      "\n",
      "Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/darraghdog/mem-check-1002-ftrlrnnverified\n",
    "# Models Packages\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time, gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.offline as plt\n",
    "import plotly.graph_objs as go\n",
    "from numba import jit\n",
    "import nltk, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "import pymorphy2\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from collections import Counter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping#, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras import initializers\n",
    "from keras.utils import plot_model\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import non_neg, Constraint\n",
    "from keras.utils.data_utils import Sequence\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "path = \"../\"\n",
    "#path = '../input/'\n",
    "path = \"/home/darragh/avito/data/\"\n",
    "#path = '/Users/dhanley2/Documents/avito/data/'\n",
    "\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/darragh/avito/nnet'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11063003540039062] Load Train/Test\n",
      "Train shape: 1503424 Rows, 17 Columns\n",
      "Test shape: 508438 Rows, 17 Columns\n",
      "[28.5412015914917] Create Validation Index\n",
      "[28.545833349227905] Combine Train and Test\n",
      "\n",
      "All Data shape: 2011862 Rows, 18 Columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "user_id                       0\n",
       "region                        0\n",
       "city                          0\n",
       "parent_category_name          0\n",
       "category_name                 0\n",
       "param_1                   84486\n",
       "param_2                  887771\n",
       "param_3                 1168896\n",
       "title                         0\n",
       "description              116276\n",
       "price                    115947\n",
       "item_seq_number               0\n",
       "activation_date               0\n",
       "user_type                     0\n",
       "image                    155197\n",
       "image_top_1              155197\n",
       "deal_probability              0\n",
       "idx                           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('[{}] Load Train/Test'.format(time.time() - start_time))\n",
    "traindf = pd.read_csv(path + 'train.csv.zip', index_col = \"item_id\", parse_dates = [\"activation_date\"], compression = 'zip')\n",
    "traindex = traindf.index\n",
    "testdf = pd.read_csv(path + 'test.csv.zip', index_col = \"item_id\", parse_dates = [\"activation_date\"])\n",
    "testdex = testdf.index\n",
    "testdf['deal_probability'] = -1\n",
    "print('Train shape: {} Rows, {} Columns'.format(*traindf.shape))\n",
    "print('Test shape: {} Rows, {} Columns'.format(*testdf.shape))\n",
    "traindf['activation_date'].value_counts()\n",
    "traindf.head()\n",
    "\n",
    "print('[{}] Create Validation Index'.format(time.time() - start_time))\n",
    "trnidx = (traindf.activation_date<=pd.to_datetime('2017-03-26')).values\n",
    "validx = (traindf.activation_date>=pd.to_datetime('2017-03-27')).values\n",
    "\n",
    "print('[{}] Combine Train and Test'.format(time.time() - start_time))\n",
    "df = pd.concat([traindf,testdf],axis=0)\n",
    "df['idx'] = range(df.shape[0])\n",
    "del traindf,testdf\n",
    "gc.collect()\n",
    "print('\\nAll Data shape: {} Rows, {} Columns'.format(*df.shape))   \n",
    "\n",
    "df.isnull().sum() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31.863198041915894] Missing values\n",
      "[32.72599959373474] Feature Engineering Price\n",
      "[33.23673343658447] Create Time Variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darragh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1    508438\n",
      " 1    327160\n",
      " 2    326892\n",
      " 0    313556\n",
      " 3    307968\n",
      " 4    227848\n",
      "Name: fold, dtype: int64\n",
      "[34.29238319396973] Text Features\n",
      "[103.8713309764862] Categoricals with some low counts\n",
      "[118.98145604133606] Encode Variables\n",
      "Label encode emb_item_seq_number\n",
      "Label encode user_id\n",
      "Label encode image_top_1\n",
      "Label encode region\n",
      "Label encode city\n",
      "Label encode emb_price\n",
      "Label encode parent_category_name\n",
      "Label encode category_name\n",
      "Label encode user_type\n",
      "Label encode emb_weekday\n",
      "Label encode text_feat\n",
      "Label encode cat_param_1\n",
      "Label encode cat_param_2\n",
      "Label encode cat_param_3\n",
      "[165.83360195159912] Scale Variables\n",
      "Scale cont_log_price\n",
      "Scale cont_log_item_seq_number\n",
      "[165.90837836265564] Embedding dimensions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darragh/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning:\n",
      "\n",
      "Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emb_item_seq_number': 6, 'user_id': 2, 'image_top_1': 9, 'region': 5, 'city': 9, 'emb_price': 6, 'parent_category_name': 4, 'category_name': 5, 'user_type': 3, 'emb_weekday': 3, 'text_feat': 9, 'cat_param_1': 7, 'cat_param_2': 7, 'cat_param_3': 8}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('[{}] Missing values'.format(time.time() - start_time))\n",
    "for col in ['param_1', 'param_2', 'param_3', 'description', 'price', 'image']:\n",
    "    df[\"bin_no_\" + col] = (df[col] != df[col]).astype(np.int32)\n",
    "cols = [c for c in df.columns if 'bin_no_' in c]\n",
    "df[cols].head()\n",
    "\n",
    "print('[{}] Feature Engineering Price'.format(time.time() - start_time))\n",
    "col = \"price\"\n",
    "bins_ = 100\n",
    "for col in ['price', 'item_seq_number']:\n",
    "    df['emb_' + col] = pd.qcut(df[col], q = bins_, labels = False, duplicates = 'drop')\n",
    "    df['emb_' + col].fillna(-1,inplace=True)\n",
    "    df['emb_' + col].value_counts()\n",
    "    df[\"cont_log_%s\"%(col)] = df[col]\n",
    "    df[\"cont_log_%s\"%(col)].fillna(-1,inplace=True)\n",
    "\n",
    "df[\"image_top_1\"].fillna(-1,inplace=True)\n",
    "\n",
    "\n",
    "print('[{}] Create Time Variables'.format(time.time() - start_time))\n",
    "df[\"emb_weekday\"] = df['activation_date'].dt.weekday\n",
    "#df[\"cont_week_of_year\"] = df['activation_date'].dt.week    <- Too different between train and test\n",
    "# df.drop([\"activation_date\",\"image\"],axis=1,inplace=True)\n",
    "\n",
    "foldls = [[\"2017-03-15\", \"2017-03-16\", \"2017-03-17\"], \\\n",
    "       [\"2017-03-18\", \"2017-03-19\", \"2017-03-20\"], \\\n",
    "       [\"2017-03-21\", \"2017-03-22\", \"2017-03-23\"], \\\n",
    "       [\"2017-03-24\", \"2017-03-25\", \"2017-03-26\"], \\\n",
    "        [\"2017-03-27\", \"2017-03-28\", \"2017-03-29\", \\\n",
    "            \"2017-03-30\", \"2017-03-31\", \"2017-04-01\", \\\n",
    "            \"2017-04-02\", \"2017-04-03\",\"2017-04-07\"]]\n",
    "foldls = [[pd.to_datetime(d) for d in f] for f in foldls]\n",
    "df['fold'] = -1\n",
    "for t, fold in enumerate(foldls):\n",
    "    df['fold'][df.activation_date.isin(fold)] = t\n",
    "df.drop([\"activation_date\",\"image\"],axis=1,inplace=True)\n",
    "print(df['fold'].value_counts())\n",
    "\n",
    "\n",
    "print('[{}] Text Features'.format(time.time() - start_time))\n",
    "df['text_feat'] = df.apply(lambda row: ' '.join([\n",
    "    str(row['param_1']), \n",
    "    str(row['param_2']), \n",
    "    str(row['param_3'])]),axis=1) # Group Param Features\n",
    "for col in ['title', 'description', 'text_feat']:\n",
    "    df[col] = df[col].str.lower()\n",
    "df['description'] = df['title'].fillna('missd') + ' ' + df[\"parent_category_name\"].fillna('') + ' ' \\\n",
    "                    + df[\"category_name\"].fillna('') + ' ' + df['text_feat'].fillna('') + \\\n",
    "                    ' ' + df['description'].fillna('')\n",
    "\n",
    "print('[{}] Categoricals with some low counts'.format(time.time() - start_time))\n",
    "def lowCtCat(col, cutoff = 20):\n",
    "    dft         = pd.DataFrame(df[col].values, columns = [col])\n",
    "    gp          = dft[col].value_counts().reset_index().rename(columns = {'index':col, col:col+'_ct'})\n",
    "    var         = dft[[col]].merge(gp, on = col, how = 'left')[col+'_ct']\n",
    "    idx         = var>cutoff\n",
    "    var[idx]    = (df[col].values)[idx]\n",
    "    var[~idx]    = 'locount'\n",
    "    var.fillna('missing', inplace = True)\n",
    "    return var.astype(str).values\n",
    "for col_, cut_ in [(\"user_id\", 5), (\"image_top_1\", 30), (\"item_seq_number\", 100)]: \n",
    "    df[col_] = lowCtCat(col_, cutoff = cut_)\n",
    "for col_, cut_ in [('param_'+str(i+1), 20) for i in range(3)]: \n",
    "    df['cat_' + col_] = lowCtCat(col_, cutoff = cut_)\n",
    "\n",
    "\n",
    "print('[{}] Encode Variables'.format(time.time() - start_time))\n",
    "embed_me = [\"emb_item_seq_number\", \"user_id\",\"image_top_1\", \"region\", 'city', 'emb_price', \\\n",
    "            \"parent_category_name\", \"category_name\", \"user_type\", \"emb_weekday\", 'text_feat'] \\\n",
    "            + ['cat_param_'+str(i+1) for i in range(3)]\n",
    "for col in embed_me:\n",
    "    print('Label encode %s'%(col))\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    df[col] = lbl.fit_transform(df[col].astype(str))\n",
    "\n",
    "print('[{}] Scale Variables'.format(time.time() - start_time))\n",
    "scl = StandardScaler()\n",
    "for col in df.columns:\n",
    "    if 'cont_' in col:\n",
    "        print('Scale %s'%(col))\n",
    "        df[col] = scl.fit_transform(df[col].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "print('[{}] Embedding dimensions'.format(time.time() - start_time))\n",
    "col_szs = dict((col, df[col].unique().shape[0]) for col in embed_me)\n",
    "embed_szs = dict((col, int(np.ceil(np.log(col_szs[col])))+1) for col in embed_me)\n",
    "embed_szs['user_id'] = 2\n",
    "print(embed_szs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[166.36902713775635] Clean text and tokenize\n",
      "Tokenise description\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('[{}] Clean text and tokenize'.format(time.time() - start_time))\n",
    "toktok = ToktokTokenizer()\n",
    "tokSentMap = {}\n",
    "stopwords_set = set(stopwords.words('russian'))\n",
    "morpher = pymorphy2.MorphAnalyzer()\n",
    "def tokSent(sent):\n",
    "   sent = sent.replace('/', ' ')\n",
    "   return \" \".join(morpher.parse(word)[0].normal_form for word in toktok.tokenize(rgx.sub(' ', sent)) if word not in stopwords_set)\n",
    "\n",
    "def tokCol(var):\n",
    "   return [tokSent(s) for s in var.tolist()]\n",
    "rgx = re.compile('[%s]' % '!\"#%&()*,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')  \n",
    "               \n",
    "\n",
    "partitions = min(cpu_count(), 8) #Define as many partitions as you want\n",
    "cores=4\n",
    "def parallelize(data, func):\n",
    "   data_split = np.array_split(data.values, partitions)\n",
    "   pool = Pool(cores)\n",
    "   data = pd.concat([pd.Series(l) for l in pool.map(tokCol, data_split)]).values\n",
    "   pool.close()\n",
    "   pool.join()\n",
    "   return data  \n",
    "\n",
    "for col in ['description', 'title',]:\n",
    "   print('Tokenise %s'%(col))\n",
    "   df[col] = parallelize(df[col], tokCol)\n",
    "print('[{}] Finished tokenizing text...'.format(time.time() - start_time))\n",
    "\n",
    "@jit\n",
    "def myTokenizerFitJit(strls, max_words, filt = True):\n",
    "    list_=[]\n",
    "    for sent in strls:\n",
    "        if filt:\n",
    "            sent = rgx.sub(' ', sent)\n",
    "        for s in sent.split(' '):\n",
    "            if s!= '':\n",
    "                list_.append(s)\n",
    "    return Counter(list_).most_common(max_words)\n",
    "\n",
    "def myTokenizerFit(strls, max_words = 25000):\n",
    "    mc = myTokenizerFitJit(strls, max_words)\n",
    "    return dict((i, c+1) for (c, (i, ii)) in enumerate(mc))  \n",
    "\n",
    "@jit\n",
    "def fit_sequence(str_, tkn_, filt = True):\n",
    "    labels = []\n",
    "    for sent in str_:\n",
    "        if filt:\n",
    "            sent = rgx.sub(' ', sent)\n",
    "        tk = []\n",
    "        for i in sent.split(' '):\n",
    "            if i in tkn_:\n",
    "                if i != '':\n",
    "                    tk.append(tkn_[i])\n",
    "        labels.append(tk)\n",
    "    return labels\n",
    "\n",
    "print('[{}] Finished FITTING TEXT DATA...'.format(time.time() - start_time))  \n",
    "tok_raw = myTokenizerFit(df['description'].loc[traindex].values.tolist()+df['title'].loc[traindex].values.tolist(), max_words = 80000)\n",
    "print('[{}] Finished PROCESSING TEXT DATA...'.format(time.time() - start_time))\n",
    "\n",
    "df[\"title\"]       = fit_sequence(df.title, tok_raw)\n",
    "df[\"description\"] = fit_sequence(df.description, tok_raw)    \n",
    "df[\"title\"]       = [l if len(l)>0 else [0] for l in df[\"title\"]]\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "MAX_DSC = max(tok_raw.values())+1\n",
    "MAX_TTL = max(tok_raw.values())+1\n",
    "\n",
    "bin_cols = [c for c in df.columns if 'bin_no' in c]\n",
    "cont_cols = [c for c in df.columns if 'cont_' in c]\n",
    "\n",
    "\n",
    "#TEST DOESNT HAVE ANY 1s\n",
    "bin_cols=[x for x in bin_cols if x!='bin_no_description']\n",
    "\n",
    "print('[{}] Finished FEATURE CREATION'.format(time.time() - start_time))  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_vectors = KeyedVectors.load_word2vec_format(path+ 'materials/wiki.ru.vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the file  https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.ru.vec\n",
    "word_vectors = KeyedVectors.load_word2vec_format(path+ 'materials/wiki.ru.vec', binary=False)\n",
    "weights= np.random.uniform(-.5,.5,MAX_DSC*300).reshape(MAX_DSC,300)\n",
    "for word, index in tok_raw.items():\n",
    "    if word in word_vectors.vocab:\n",
    "        weights[index] = word_vectors.get_vector(word)\n",
    "del word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_sort(seq1, seq2):\n",
    "\treturn sorted(range(len(seq1)), key=lambda x: max(len(seq1[x]),len(seq2[x])))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class Seq_generator(Sequence):\n",
    "    def __init__(self, dt, bsize, sort_vals, target_out=True):        \n",
    "        self.batch_size = bsize\n",
    "        self.dt = dt.iloc[sort_vals].reset_index(drop=True)\n",
    "        if target_out:\n",
    "            self.y = self.dt.deal_probability.values\n",
    "        else:\n",
    "            self.y = None\n",
    "    \n",
    "    def get_keras_data(self, dataset):\n",
    "        X = {\n",
    "            'title': pad_sequences(dataset.title, \n",
    "                                  maxlen=max([len(l) for l in dataset.title]))\n",
    "            ,'description': pad_sequences(dataset.description, \n",
    "                                  maxlen=max([len(l) for l in dataset.description]))\n",
    "            }\n",
    "        for col in embed_szs.keys():\n",
    "            X[col] = dataset[col].values\n",
    "        X['bin_vars'] = dataset[bin_cols].values\n",
    "        X['cont_vars'] = dataset[cont_cols].values\n",
    "        return X   \n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.dt.shape[0]*1./self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        slc = slice(i*self.batch_size, min((i+1)*self.batch_size, self.dt.shape[0]))\n",
    "        X = self.get_keras_data(self.dt.iloc[slc])\n",
    "        if self.y is not None:\n",
    "            return (X, self.y[slc])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_model(emb_size = 32, dr = 0.1, l2_val = 0.0001):\n",
    "    \n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "    \n",
    "    \n",
    "    def mean_error(y_true, y_pred):\n",
    "        return K.mean(y_true-y_pred)\n",
    "    \n",
    "    def repeat_smart(x):\n",
    "        return K.repeat(x[0], K.shape(x[1])[1])\n",
    "    \n",
    "    class FreezePadding(Constraint):\n",
    "        \"\"\"Freezes the last weight to be near 0.\"\"\"\n",
    "        def __call__(self, w):\n",
    "            other_weights = K.cast(K.ones(K.shape(w))[1:], K.floatx())\n",
    "            last_weight = K.cast(K.equal(K.reshape(w[0, :], (1, K.shape(w)[1])), 0.), K.floatx())\n",
    "            appended = K.concatenate([other_weights, last_weight], axis=0)\n",
    "            w *= appended\n",
    "            return w\n",
    "\n",
    "    ##Inputs\n",
    "    title = Input(shape=[None], name=\"title\")\n",
    "    description = Input(shape=[None], name=\"description\")\n",
    "    \n",
    "    # Categorical embeddings\n",
    "    emb_inputs = dict((col, Input(shape=[1], name = col))  for col in embed_szs.keys())\n",
    "    emb_model  = dict((col, Embedding(col_szs[col]+1, emb_n, embeddings_regularizer=l2(l2_val))(emb_inputs[col])) for (col, emb_n) in embed_szs.items())\n",
    "    fe = concatenate([(emb_) for emb_ in emb_model.values()])\n",
    "    #fe = SpatialDropout1D(dr)(fe)\n",
    "    \n",
    "    # Binary Inputs\n",
    "    bin_vars = Input(shape= [len(bin_cols)], name = 'bin_vars')\n",
    "    ## Continuous Inputs\n",
    "    cont_vars = Input(shape= [len(cont_cols)], name = 'cont_vars')\n",
    "    \n",
    "    \n",
    "\n",
    "    #Embeddings layers\n",
    "    embs_text = Embedding(MAX_DSC, 300, embeddings_regularizer=l2(l2_val), name='text_emb', \n",
    "                          embeddings_constraint=FreezePadding(), weights= [weights])\n",
    "    emb_dsc = embs_text(description) \n",
    "    emb_ttl = embs_text(title)\n",
    "    \n",
    "    \n",
    "#     static_features = concatenate([Flatten()(fe)\n",
    "#                                    , bin_vars\n",
    "#                                    , cont_vars])\n",
    "    \n",
    "#     emb_dsc = concatenate([emb_dsc, Lambda(repeat_smart)([static_features, emb_dsc])])\n",
    "#     emb_ttl = concatenate([emb_ttl, Lambda(repeat_smart)([static_features, emb_ttl])])\n",
    "    \n",
    "    # GRU Layer\n",
    "    rnn_dsc = (CuDNNGRU(emb_size))(emb_dsc)\n",
    "    rnn_ttl = (CuDNNGRU(emb_size))(emb_ttl)\n",
    "    \n",
    "    #main layer\n",
    "    main_l = concatenate([\n",
    "        rnn_dsc\n",
    "        , rnn_ttl\n",
    "        , Flatten()(fe)\n",
    "        , bin_vars\n",
    "        , cont_vars\n",
    "    ])\n",
    "    \n",
    "    main_l = Dense(32, kernel_regularizer=l2(l2_val)) (main_l)\n",
    "    main_l = PReLU()(main_l)\n",
    "    #main_l = BatchNormalization()(main_l)\n",
    "    main_l = Dropout(dr)(main_l)\n",
    "    main_l = Dense(16, kernel_regularizer=l2(l2_val)) (main_l)\n",
    "    main_l = PReLU()(main_l)\n",
    "    #main_l = BatchNormalization()(main_l)\n",
    "    main_l = Dropout(dr/2)(main_l)\n",
    "    \n",
    "    #output\n",
    "    output = Dense(1,activation=\"linear\", kernel_regularizer=l2(l2_val)) (main_l)\n",
    "    \n",
    "    #model\n",
    "    model = Model([title, description] + [inp for inp in emb_inputs.values()] + [bin_vars] + [cont_vars], output)\n",
    "    optimizer = optimizers.Adam()\n",
    "    model.compile(loss=root_mean_squared_error, \n",
    "                  optimizer=optimizer, metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dtrain, dvalid):  \n",
    "    epochs = 26\n",
    "    batchSize = 512\n",
    "    steps = (dtrain.shape[0]/batchSize+1)*epochs\n",
    "    lr_init, lr_fin = 0.0014, 0.00001\n",
    "    lr_decay  = (lr_init - lr_fin)/steps\n",
    "\n",
    "    bags      = 3\n",
    "    y_pred_ls = []\n",
    "    train_sorted_ix = np.array(map_sort(dtrain[\"title\"].tolist(), dtrain[\"description\"].tolist()))\n",
    "    val_sorted_ix = np.array(map_sort(dvalid[\"title\"].tolist(), dvalid[\"description\"].tolist()))\n",
    "    for b in range(bags):\n",
    "        model = get_model(64, .1,.00001)\n",
    "        K.set_value(model.optimizer.lr, lr_init)\n",
    "        K.set_value(model.optimizer.decay, lr_decay)\n",
    "        #model.summary()\n",
    "        for i in range(epochs):\n",
    "            batchSize = min(512*(2**i),512)\n",
    "            batchSizeTst = 256\n",
    "            history = model.fit_generator(\n",
    "                                Seq_generator(dtrain, batchSize, train_sorted_ix)\n",
    "                                , epochs=i+1\n",
    "                                , max_queue_size=15\n",
    "                                , verbose=2\n",
    "                                , initial_epoch=i\n",
    "                                , use_multiprocessing=False\n",
    "                                , workers=1\n",
    "                                )\n",
    "            y_pred_ls.append(model.predict_generator(\n",
    "                             Seq_generator(dvalid, batchSizeTst, val_sorted_ix, target_out=False)\n",
    "                            , max_queue_size=10\n",
    "                            , verbose=2)[val_sorted_ix.argsort()])\n",
    "            print('RMSE:', np.sqrt(metrics.mean_squared_error( dvalid['target'], y_pred_ls[-1])))\n",
    "\n",
    "                \n",
    "    def to_logit(ls):\n",
    "        ls=np.array(ls)\n",
    "        ls=np.clip(ls,.0001,.9999)\n",
    "        return np.log(ls/(1-ls))\n",
    "\n",
    "    def to_proba(ls):\n",
    "        return 1/(1+np.exp(-ls))\n",
    "    \n",
    "    \n",
    "    res = np.full((epochs,epochs+1),1.)\n",
    "    for i in range(epochs):\n",
    "        for j in range(i+1,epochs+1):\n",
    "            preds = sum([sum(to_logit(y_pred_ls[i+epochs*bag:j+epochs*bag]))/len(y_pred_ls[i+epochs*bag:j+epochs*bag]) for bag in range(bags)])/bags\n",
    "            res[i,j] = np.sqrt(metrics.mean_squared_error(dvalid['target'], to_proba(preds.flatten())))\n",
    "    #         print(i,' to ',j, 'RMSE bags:', res[i,j])\n",
    "\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print(i,' ',np.argsort(res)[i,0], ':', res[i,np.argsort(res)[i,0]])\n",
    "\n",
    "    \n",
    "    i=input(\"Start\")\n",
    "    j=input(\"Finish\")\n",
    "    y_sub = to_proba(sum([sum(to_logit(y_pred_ls[i+epochs*bag:j+epochs*bag]))/len(y_pred_ls[i+epochs*bag:j+epochs*bag]) for bag in range(bags)])/bags)    \n",
    "\n",
    "    return y_sub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['rnn_preds'] = 0.\n",
    "for f in range(4,5):\n",
    "    print(\"DOING FOLD {}\".format(f))\n",
    "    if f==5:\n",
    "        trnidx = (df.fold!=-1)\n",
    "        validx = df.fold==-1\n",
    "    else:\n",
    "        trnidx = (df.fold!=-1) & (df.fold!=f)\n",
    "        validx = (df.fold!=-1) & (df.fold==f)\n",
    "    \n",
    "    dtrain = df[trnidx]\n",
    "    dvalid = df[validx]\n",
    "    df['rnn_preds'][validx] = train_model(dtrain, dvalid)\n",
    "    print('RMSE:', np.sqrt(metrics.mean_squared_error( df[validx]['deal_probability'], df[validx]['rnn_preds'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['rnn_preds']].to_csv('../sub/rnnCV_2805.csv.gz', index=True, header=True, compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
