{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rosenflanzt/anaconda3/envs/kaggle_avito/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning:\n",
      "\n",
      "Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/darraghdog/mem-check-1002-ftrlrnnverified\n",
    "# Models Packages\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time, gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.offline as plt\n",
    "import plotly.graph_objs as go\n",
    "from numba import jit\n",
    "import nltk, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "import pymorphy2\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from collections import Counter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping#, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras import initializers\n",
    "from keras.utils import plot_model\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import non_neg, Constraint\n",
    "from keras.utils.data_utils import Sequence\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "path = \"../\"\n",
    "\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.005577802658081055] Load Train/Test\n"
     ]
    }
   ],
   "source": [
    "print('[{}] Load Train/Test'.format(time.time() - start_time))\n",
    "traindf = pd.read_csv(path + 'train.csv.zip', index_col = \"item_id\", parse_dates = [\"activation_date\"], compression = 'zip')\n",
    "traindex = traindf.index\n",
    "testdf = pd.read_csv(path + 'test.csv.zip', index_col = \"item_id\", parse_dates = [\"activation_date\"])\n",
    "testdex = testdf.index\n",
    "testdf['deal_probability'] = -1\n",
    "print('Train shape: {} Rows, {} Columns'.format(*traindf.shape))\n",
    "print('Test shape: {} Rows, {} Columns'.format(*testdf.shape))\n",
    "traindf['activation_date'].value_counts()\n",
    "traindf.head()\n",
    "\n",
    "print('[{}] Create Validation Index'.format(time.time() - start_time))\n",
    "trnidx = (traindf.activation_date<=pd.to_datetime('2017-03-26')).values\n",
    "validx = (traindf.activation_date>=pd.to_datetime('2017-03-27')).values\n",
    "\n",
    "print('[{}] Combine Train and Test'.format(time.time() - start_time))\n",
    "df = pd.concat([traindf,testdf],axis=0)\n",
    "df['idx'] = range(df.shape[0])\n",
    "del traindf,testdf\n",
    "gc.collect()\n",
    "print('\\nAll Data shape: {} Rows, {} Columns'.format(*df.shape))   \n",
    "\n",
    "df.isnull().sum() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('[{}] Missing values'.format(time.time() - start_time))\n",
    "for col in ['param_1', 'param_2', 'param_3', 'description', 'price', 'image']:\n",
    "    df[\"bin_no_\" + col] = (df[col] != df[col]).astype(np.int32)\n",
    "cols = [c for c in df.columns if 'bin_no_' in c]\n",
    "df[cols].head()\n",
    "\n",
    "print('[{}] Feature Engineering Price'.format(time.time() - start_time))\n",
    "col = \"price\"\n",
    "bins_ = 100\n",
    "for col in ['price', 'item_seq_number']:\n",
    "    df['emb_' + col] = pd.qcut(df[col], q = bins_, labels = False, duplicates = 'drop')\n",
    "    df['emb_' + col].fillna(-1,inplace=True)\n",
    "    df['emb_' + col].value_counts()\n",
    "    df[\"cont_log_%s\"%(col)] = df[col]\n",
    "    df[\"cont_log_%s\"%(col)].fillna(-1,inplace=True)\n",
    "\n",
    "df[\"image_top_1\"].fillna(-1,inplace=True)\n",
    "\n",
    "\n",
    "print('[{}] Create Time Variables'.format(time.time() - start_time))\n",
    "df[\"emb_weekday\"] = df['activation_date'].dt.weekday\n",
    "#df[\"cont_week_of_year\"] = df['activation_date'].dt.week    <- Too different between train and test\n",
    "# df.drop([\"activation_date\",\"image\"],axis=1,inplace=True)\n",
    "\n",
    "foldls = [[\"2017-03-15\", \"2017-03-16\", \"2017-03-17\"], \\\n",
    "       [\"2017-03-18\", \"2017-03-19\", \"2017-03-20\"], \\\n",
    "       [\"2017-03-21\", \"2017-03-22\", \"2017-03-23\"], \\\n",
    "       [\"2017-03-24\", \"2017-03-25\", \"2017-03-26\"], \\\n",
    "        [\"2017-03-27\", \"2017-03-28\", \"2017-03-29\", \\\n",
    "            \"2017-03-30\", \"2017-03-31\", \"2017-04-01\", \\\n",
    "            \"2017-04-02\", \"2017-04-03\",\"2017-04-07\"]]\n",
    "foldls = [[pd.to_datetime(d) for d in f] for f in foldls]\n",
    "df['fold'] = -1\n",
    "for t, fold in enumerate(foldls):\n",
    "    df['fold'][df.activation_date.isin(fold)] = t\n",
    "df.drop([\"activation_date\",\"image\"],axis=1,inplace=True)\n",
    "print(df['fold'].value_counts())\n",
    "\n",
    "\n",
    "print('[{}] Text Features'.format(time.time() - start_time))\n",
    "df['text_feat'] = df.apply(lambda row: ' '.join([\n",
    "    str(row['param_1']), \n",
    "    str(row['param_2']), \n",
    "    str(row['param_3'])]),axis=1) # Group Param Features\n",
    "for col in ['title', 'description', 'text_feat']:\n",
    "    df[col] = df[col].str.lower()\n",
    "df['description'] = df['title'].fillna('missd') + ' ' + df[\"parent_category_name\"].fillna('') + ' ' \\\n",
    "                    + df[\"category_name\"].fillna('') + ' ' + df['text_feat'].fillna('') + \\\n",
    "                    ' ' + df['description'].fillna('')\n",
    "\n",
    "print('[{}] Categoricals with some low counts'.format(time.time() - start_time))\n",
    "def lowCtCat(col, cutoff = 20):\n",
    "    dft         = pd.DataFrame(df[col].values, columns = [col])\n",
    "    gp          = dft[col].value_counts().reset_index().rename(columns = {'index':col, col:col+'_ct'})\n",
    "    var         = dft[[col]].merge(gp, on = col, how = 'left')[col+'_ct']\n",
    "    idx         = var>cutoff\n",
    "    var[idx]    = (df[col].values)[idx]\n",
    "    var[~idx]    = 'locount'\n",
    "    var.fillna('missing', inplace = True)\n",
    "    return var.astype(str).values\n",
    "for col_, cut_ in [(\"user_id\", 5), (\"image_top_1\", 30), (\"item_seq_number\", 100)]: \n",
    "    df[col_] = lowCtCat(col_, cutoff = cut_)\n",
    "for col_, cut_ in [('param_'+str(i+1), 20) for i in range(3)]: \n",
    "    df['cat_' + col_] = lowCtCat(col_, cutoff = cut_)\n",
    "\n",
    "\n",
    "print('[{}] Encode Variables'.format(time.time() - start_time))\n",
    "embed_me = [\"emb_item_seq_number\", \"user_id\",\"image_top_1\", \"region\", 'city', 'emb_price', \\\n",
    "            \"parent_category_name\", \"category_name\", \"user_type\", \"emb_weekday\", 'text_feat'] \\\n",
    "            + ['cat_param_'+str(i+1) for i in range(3)]\n",
    "for col in embed_me:\n",
    "    print('Label encode %s'%(col))\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    df[col] = lbl.fit_transform(df[col].astype(str))\n",
    "\n",
    "print('[{}] Scale Variables'.format(time.time() - start_time))\n",
    "scl = StandardScaler()\n",
    "for col in df.columns:\n",
    "    if 'cont_' in col:\n",
    "        print('Scale %s'%(col))\n",
    "        df[col] = scl.fit_transform(df[col].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "print('[{}] Embedding dimensions'.format(time.time() - start_time))\n",
    "col_szs = dict((col, df[col].unique().shape[0]) for col in embed_me)\n",
    "embed_szs = dict((col, int(np.ceil(np.log(col_szs[col])))+1) for col in embed_me)\n",
    "embed_szs['user_id'] = 2\n",
    "print(embed_szs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('[{}] Clean text and tokenize'.format(time.time() - start_time))\n",
    "toktok = ToktokTokenizer()\n",
    "tokSentMap = {}\n",
    "stopwords_set = set(stopwords.words('russian'))\n",
    "morpher = pymorphy2.MorphAnalyzer()\n",
    "def tokSent(sent):\n",
    "   sent = sent.replace('/', ' ')\n",
    "   return \" \".join(morpher.parse(word)[0].normal_form for word in toktok.tokenize(rgx.sub(' ', sent)) if word not in stopwords_set)\n",
    "\n",
    "def tokCol(var):\n",
    "   return [tokSent(s) for s in var.tolist()]\n",
    "rgx = re.compile('[%s]' % '!\"#%&()*,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')  \n",
    "               \n",
    "\n",
    "partitions = min(cpu_count(), 8) #Define as many partitions as you want\n",
    "cores=4\n",
    "def parallelize(data, func):\n",
    "   data_split = np.array_split(data.values, partitions)\n",
    "   pool = Pool(cores)\n",
    "   data = pd.concat([pd.Series(l) for l in pool.map(tokCol, data_split)]).values\n",
    "   pool.close()\n",
    "   pool.join()\n",
    "   return data  \n",
    "\n",
    "for col in ['description', 'title',]:\n",
    "   print('Tokenise %s'%(col))\n",
    "   df[col] = parallelize(df[col], tokCol)\n",
    "print('[{}] Finished tokenizing text...'.format(time.time() - start_time))\n",
    "\n",
    "@jit\n",
    "def myTokenizerFitJit(strls, max_words, filt = True):\n",
    "    list_=[]\n",
    "    for sent in strls:\n",
    "        if filt:\n",
    "            sent = rgx.sub(' ', sent)\n",
    "        for s in sent.split(' '):\n",
    "            if s!= '':\n",
    "                list_.append(s)\n",
    "    return Counter(list_).most_common(max_words)\n",
    "\n",
    "def myTokenizerFit(strls, max_words = 25000):\n",
    "    mc = myTokenizerFitJit(strls, max_words)\n",
    "    return dict((i, c+1) for (c, (i, ii)) in enumerate(mc))  \n",
    "\n",
    "@jit\n",
    "def fit_sequence(str_, tkn_, filt = True):\n",
    "    labels = []\n",
    "    for sent in str_:\n",
    "        if filt:\n",
    "            sent = rgx.sub(' ', sent)\n",
    "        tk = []\n",
    "        for i in sent.split(' '):\n",
    "            if i in tkn_:\n",
    "                if i != '':\n",
    "                    tk.append(tkn_[i])\n",
    "        labels.append(tk)\n",
    "    return labels\n",
    "\n",
    "print('[{}] Finished FITTING TEXT DATA...'.format(time.time() - start_time))  \n",
    "tok_raw = myTokenizerFit(df['description'].loc[traindex].values.tolist()+df['title'].loc[traindex].values.tolist(), max_words = 80000)\n",
    "print('[{}] Finished PROCESSING TEXT DATA...'.format(time.time() - start_time))\n",
    "\n",
    "df[\"title\"]       = fit_sequence(df.title, tok_raw)\n",
    "df[\"description\"] = fit_sequence(df.description, tok_raw)    \n",
    "df[\"title\"]       = [l if len(l)>0 else [0] for l in df[\"title\"]]\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "MAX_DSC = max(tok_raw.values())+1\n",
    "MAX_TTL = max(tok_raw.values())+1\n",
    "\n",
    "bin_cols = [c for c in df.columns if 'bin_no' in c]\n",
    "cont_cols = [c for c in df.columns if 'cont_' in c]\n",
    "\n",
    "\n",
    "#TEST DOESNT HAVE ANY 1s\n",
    "bin_cols=[x for x in bin_cols if x!='bin_no_description']\n",
    "\n",
    "print('[{}] Finished FEATURE CREATION'.format(time.time() - start_time))  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format(path+ 'materials/wiki.ru.vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the file  https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.ru.vec\n",
    "word_vectors = KeyedVectors.load_word2vec_format(path+ 'materials/wiki.ru.vec', binary=False)\n",
    "weights= np.random.uniform(-.5,.5,MAX_DSC*300).reshape(MAX_DSC,300)\n",
    "for word, index in tok_raw.items():\n",
    "    if word in word_vectors.vocab:\n",
    "        weights[index] = word_vectors.get_vector(word)\n",
    "del word_vectors\n",
    "K.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_sort(seq1, seq2):\n",
    "\treturn sorted(range(len(seq1)), key=lambda x: max(len(seq1[x]),len(seq2[x])))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class Seq_generator(Sequence):\n",
    "    def __init__(self, dt, bsize, sort_vals, target_out=True):        \n",
    "        self.batch_size = bsize\n",
    "        self.dt = dt.iloc[sort_vals].reset_index(drop=True)\n",
    "        if target_out:\n",
    "            self.y = self.dt.deal_probability.values\n",
    "        else:\n",
    "            self.y = None\n",
    "    \n",
    "    def get_keras_data(self, dataset):\n",
    "        X = {\n",
    "            'title': pad_sequences(dataset.title, \n",
    "                                  maxlen=max([len(l) for l in dataset.title]))\n",
    "            ,'description': pad_sequences(dataset.description, \n",
    "                                  maxlen=max([len(l) for l in dataset.description]))\n",
    "            }\n",
    "        for col in embed_szs.keys():\n",
    "            X[col] = dataset[col].values\n",
    "        X['bin_vars'] = dataset[bin_cols].values\n",
    "        X['cont_vars'] = dataset[cont_cols].values\n",
    "        return X   \n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.dt.shape[0]*1./self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        slc = slice(i*self.batch_size, min((i+1)*self.batch_size, self.dt.shape[0]))\n",
    "        X = self.get_keras_data(self.dt.iloc[slc])\n",
    "        if self.y is not None:\n",
    "            return (X, self.y[slc])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.362799882888794"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_model(emb_size = 32, dr = 0.1, l2_val = 0.0001):\n",
    "    \n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "    \n",
    "    \n",
    "    def mean_error(y_true, y_pred):\n",
    "        return K.mean(y_true-y_pred)\n",
    "    \n",
    "    def repeat_smart(x):\n",
    "        return K.repeat(x[0], K.shape(x[1])[1])\n",
    "    \n",
    "    class FreezePadding(Constraint):\n",
    "        \"\"\"Freezes the last weight to be near 0.\"\"\"\n",
    "        def __call__(self, w):\n",
    "            other_weights = K.cast(K.ones(K.shape(w))[1:], K.floatx())\n",
    "            last_weight = K.cast(K.equal(K.reshape(w[0, :], (1, K.shape(w)[1])), 0.), K.floatx())\n",
    "            appended = K.concatenate([last_weight,other_weights], axis=0)\n",
    "            w *= appended\n",
    "            return w\n",
    "\n",
    "    ##Inputs\n",
    "    title = Input(shape=[None], name=\"title\")\n",
    "    description = Input(shape=[None], name=\"description\")\n",
    "    \n",
    "    # Categorical embeddings\n",
    "    emb_inputs = dict((col, Input(shape=[1], name = col))  for col in embed_szs.keys())\n",
    "    emb_model  = dict((col, Embedding(col_szs[col]+1, emb_n, embeddings_regularizer=l2(l2_val))(emb_inputs[col])) for (col, emb_n) in embed_szs.items())\n",
    "    fe = concatenate([(emb_) for emb_ in emb_model.values()])\n",
    "    #fe = SpatialDropout1D(dr)(fe)\n",
    "    \n",
    "    # Binary Inputs\n",
    "    bin_vars = Input(shape= [len(bin_cols)], name = 'bin_vars')\n",
    "    ## Continuous Inputs\n",
    "    cont_vars = Input(shape= [len(cont_cols)], name = 'cont_vars')\n",
    "    \n",
    "    \n",
    "\n",
    "    #Embeddings layers\n",
    "    embs_text = Embedding(MAX_DSC, 300, embeddings_regularizer=l2(l2_val/10), name='text_emb', \n",
    "                          embeddings_constraint=FreezePadding(), weights= [weights])\n",
    "    emb_dsc = embs_text(description) \n",
    "    emb_ttl = embs_text(title)\n",
    "    \n",
    "    \n",
    "#     static_features = concatenate([Flatten()(fe)\n",
    "#                                    , bin_vars\n",
    "#                                    , cont_vars])\n",
    "    \n",
    "#     emb_dsc = concatenate([emb_dsc, Lambda(repeat_smart)([static_features, emb_dsc])])\n",
    "#     emb_ttl = concatenate([emb_ttl, Lambda(repeat_smart)([static_features, emb_ttl])])\n",
    "    \n",
    "    # GRU Layer\n",
    "    rnn_dsc = (CuDNNGRU(emb_size))(emb_dsc)\n",
    "    rnn_ttl = (CuDNNGRU(emb_size))(emb_ttl)\n",
    "    \n",
    "    #main layer\n",
    "    main_l = concatenate([\n",
    "        rnn_dsc\n",
    "        , rnn_ttl\n",
    "        , Flatten()(fe)\n",
    "        , bin_vars\n",
    "        , cont_vars\n",
    "    ])\n",
    "    \n",
    "    main_l = Dense(32, kernel_regularizer=l2(l2_val)) (main_l)\n",
    "    main_l = PReLU()(main_l)\n",
    "    #main_l = BatchNormalization()(main_l)\n",
    "    main_l = Dropout(dr)(main_l)\n",
    "    main_l = Dense(16, kernel_regularizer=l2(l2_val)) (main_l)\n",
    "    main_l = PReLU()(main_l)\n",
    "    #main_l = BatchNormalization()(main_l)\n",
    "    main_l = Dropout(dr/2)(main_l)\n",
    "    \n",
    "    #output\n",
    "    output = Dense(1,activation=\"linear\", kernel_regularizer=l2(l2_val)) (main_l)\n",
    "    \n",
    "    #model\n",
    "    model = Model([title, description] + [inp for inp in emb_inputs.values()] + [bin_vars] + [cont_vars], output)\n",
    "    optimizer = optimizers.Adam()\n",
    "    model.compile(loss=root_mean_squared_error, \n",
    "                  optimizer=optimizer, metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "def train_model(dtrain, dvalid): \n",
    "    def to_logit(ls):\n",
    "        ls=np.array(ls)\n",
    "        ls=np.clip(ls,.0001,.9999)\n",
    "        return np.log(ls/(1-ls))\n",
    "\n",
    "    def to_proba(ls):\n",
    "        return 1/(1+np.exp(-ls))\n",
    "    epochs = 26\n",
    "    batchSize = 256\n",
    "    steps = (dtrain.shape[0]/batchSize+1)*epochs\n",
    "    lr_init, lr_fin = 0.0014, 0.00001\n",
    "    lr_decay  = (lr_init - lr_fin)/steps\n",
    "\n",
    "    bags      = 3\n",
    "    y_pred_ls = []\n",
    "    train_sorted_ix = np.array(map_sort(dtrain[\"title\"].tolist(), dtrain[\"description\"].tolist()))\n",
    "    val_sorted_ix = np.array(map_sort(dvalid[\"title\"].tolist(), dvalid[\"description\"].tolist()))\n",
    "    for b in range(bags):\n",
    "        model = get_model(32, .2,.0001)\n",
    "        K.set_value(model.optimizer.lr, lr_init)\n",
    "        K.set_value(model.optimizer.decay, lr_decay)\n",
    "        #model.summary()\n",
    "        for i in range(epochs):\n",
    "            batchSize = min(512*(2**i),256)\n",
    "            batchSizeTst = 256\n",
    "            history = model.fit_generator(\n",
    "                                Seq_generator(dtrain, batchSize, train_sorted_ix)\n",
    "                                , epochs=i+1\n",
    "                                , max_queue_size=15\n",
    "                                , verbose=1\n",
    "                                , initial_epoch=i\n",
    "                                , use_multiprocessing=False\n",
    "                                , workers=1\n",
    "                                )\n",
    "            y_pred_ls.append(model.predict_generator(\n",
    "                             Seq_generator(dvalid, batchSizeTst, val_sorted_ix, target_out=False)\n",
    "                            , max_queue_size=10\n",
    "                            , verbose=2)[val_sorted_ix.argsort()])\n",
    "            print('RMSE:', np.sqrt(metrics.mean_squared_error( dvalid['deal_probability'], y_pred_ls[-1])))\n",
    "            preds = sum(to_logit(y_pred_ls)/len(y_pred_ls))\n",
    "            print('RMSE-BAG:', np.sqrt(metrics.mean_squared_error(dvalid['deal_probability'], to_proba(preds.flatten()))))\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "                \n",
    "   \n",
    "    \n",
    "    res = np.full((epochs,epochs+1),1.)\n",
    "    for i in range(epochs):\n",
    "        for j in range(i+1,epochs+1):\n",
    "            preds = sum([sum(to_logit(y_pred_ls[i+epochs*bag:j+epochs*bag]))/len(y_pred_ls[i+epochs*bag:j+epochs*bag]) for bag in range(bags)])/bags\n",
    "            res[i,j] = np.sqrt(metrics.mean_squared_error(dvalid['deal_probability'], to_proba(preds.flatten())))\n",
    "    #         print(i,' to ',j, 'RMSE bags:', res[i,j])\n",
    "\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print(i,' ',np.argsort(res)[i,0], ':', res[i,np.argsort(res)[i,0]])\n",
    "\n",
    "    \n",
    "    i=3  #Change these if you see better options printed \n",
    "    j=26\n",
    "    y_sub = to_proba(sum([sum(to_logit(y_pred_ls[i+epochs*bag:j+epochs*bag]))/len(y_pred_ls[i+epochs*bag:j+epochs*bag]) for bag in range(bags)])/bags)    \n",
    "\n",
    "    return y_sub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOING FOLD 5\n",
      "Epoch 1/1\n",
      "5873/5873 [==============================] - 451s 77ms/step - loss: 0.7778 - mean_absolute_error: 0.1549\n",
      "RMSE: 1.1548157660453406\n",
      "RMSE-BAG: 1.1548145866664246\n",
      "Epoch 2/2\n",
      "5873/5873 [==============================] - 448s 76ms/step - loss: 0.2349 - mean_absolute_error: 0.1510\n",
      "RMSE: 1.1426929314372352\n",
      "RMSE-BAG: 1.147785428703518\n",
      "Epoch 3/3\n",
      "5873/5873 [==============================] - 452s 77ms/step - loss: 0.2333 - mean_absolute_error: 0.1501\n",
      "RMSE: 1.1474617527501745\n",
      "RMSE-BAG: 1.147077122551178\n",
      "Epoch 4/4\n",
      "5873/5873 [==============================] - 457s 78ms/step - loss: 0.2328 - mean_absolute_error: 0.1496\n",
      "RMSE: 1.150164596268436\n",
      "RMSE-BAG: 1.1473126311734696\n",
      "Epoch 5/5\n",
      "5873/5873 [==============================] - 456s 78ms/step - loss: 0.2331 - mean_absolute_error: 0.1493\n",
      "RMSE: 1.1407106666757822\n",
      "RMSE-BAG: 1.1454677509358266\n",
      "Epoch 6/6\n",
      "5873/5873 [==============================] - 457s 78ms/step - loss: 0.2319 - mean_absolute_error: 0.1487\n",
      "RMSE: 1.1432837159356488\n",
      "RMSE-BAG: 1.1447677252392008\n",
      "Epoch 7/7\n",
      "4641/5873 [======================>.......] - ETA: 1:35 - loss: 0.2315 - mean_absolute_error: 0.1484"
     ]
    }
   ],
   "source": [
    "\n",
    "df['rnn_preds'] = -1.\n",
    "for f in range(5,6):\n",
    "    print(\"DOING FOLD {}\".format(f))\n",
    "    if f==5:\n",
    "        trnidx = (df.fold!=-1)\n",
    "        validx = df.fold==-1\n",
    "    else:\n",
    "        trnidx = (df.fold!=-1) & (df.fold!=f)\n",
    "        validx = (df.fold!=-1) & (df.fold==f)\n",
    "    \n",
    "    dtrain = df[trnidx]\n",
    "    dvalid = df[validx]\n",
    "    df['rnn_preds'][validx] = train_model(dtrain, dvalid)\n",
    "    print('RMSE:', np.sqrt(metrics.mean_squared_error( df[validx]['deal_probability'], df[validx]['rnn_preds'])))\n",
    "    K.clear_session()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['deal_probability']==0][['rnn_preds']].to_csv('../sub/rnnFT_0106.csv.gz', index=True, header=True, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
